{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b21943",
   "metadata": {},
   "source": [
    "## 1 - BERT Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb8acf6",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "killing-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "private-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "timely-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce550d",
   "metadata": {},
   "source": [
    "## Loading the Vocabulary File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "controlled-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocab = pd.read_csv(\"vocab.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "organized-prompt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vocab[\"Word\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad77495",
   "metadata": {},
   "source": [
    "## Adding the CLS and SEP token to word in the vocab list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hybrid-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"[CLS] \" + df_vocab[\"Word\"][0] + \" [SEP]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11d272a",
   "metadata": {},
   "source": [
    "## Tokenize the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "controlled-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_text = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "accurate-sympathy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'a', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75465094",
   "metadata": {},
   "source": [
    "## Getting indexes of the items in the tokenized text list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "recognized-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenizer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "willing-parks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1037, 102]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "center-cleaners",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[1037]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db74472a",
   "metadata": {},
   "source": [
    "## Returning pair of tokenized text item list and their corresponding index in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "labeled-connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 101\n",
      "a 1037\n",
      "[SEP] 102\n"
     ]
    }
   ],
   "source": [
    "for text, index in zip(tokenizer_text, indexed_tokens):\n",
    "    print(text,index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc1256c",
   "metadata": {},
   "source": [
    "## Creating Segmentation IDs for the tokenzied text, since we all have single words, it'll be set to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "strange-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_ids = [1] * len(tokenizer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "local-onion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236bed23",
   "metadata": {},
   "source": [
    "## Converting list of tokens and segments to their corresponding tensor to pass through the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "under-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6409df",
   "metadata": {},
   "source": [
    "## Loading the pre-trained BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "arbitrary-transfer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600fa18",
   "metadata": {},
   "source": [
    "## Testing single word embedding output from the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "automotive-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a77e6f8",
   "metadata": {},
   "source": [
    "## Printing hidden state tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "decreased-appeal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "          [ 0.5956,  0.5420,  0.0412,  ...,  0.4376,  0.5639,  0.3365],\n",
       "          [-0.4815, -0.0189,  0.0092,  ..., -0.2806,  0.3895, -0.2815]]]),\n",
       " tensor([[[ 0.1383, -0.0017, -0.1345,  ...,  0.0213, -0.0027,  0.0546],\n",
       "          [ 0.7541,  0.5487,  0.1515,  ...,  0.2371,  0.1910,  0.0636],\n",
       "          [-0.3558,  0.3080, -0.1537,  ..., -0.4325,  0.8373, -0.1724]]]),\n",
       " tensor([[[-0.0382, -0.1733, -0.1631,  ...,  0.0534,  0.0795,  0.0761],\n",
       "          [ 0.3919,  0.5963,  0.4106,  ...,  0.5559,  0.2573,  0.3806],\n",
       "          [-0.3174,  0.0915,  0.0514,  ..., -0.2584,  0.7475, -0.0926]]]),\n",
       " tensor([[[ 0.0025, -0.2923, -0.0254,  ...,  0.2050,  0.1381,  0.2798],\n",
       "          [ 0.2570,  0.4708,  0.6375,  ...,  0.5920,  0.1055, -0.1956],\n",
       "          [-0.0878, -0.0809,  0.0926,  ..., -0.0043,  0.1657,  0.0037]]]),\n",
       " tensor([[[ 0.0535, -0.4204, -0.5296,  ...,  0.3501, -0.0072,  0.6119],\n",
       "          [ 0.1004,  0.4128, -0.0307,  ..., -0.1951, -0.0311, -0.2469],\n",
       "          [-0.0349, -0.0409,  0.0159,  ..., -0.0076,  0.0641, -0.0299]]]),\n",
       " tensor([[[-0.5462, -0.4105, -0.6200,  ..., -0.1005,  0.1239,  0.7218],\n",
       "          [-0.0299,  0.3036, -0.1488,  ..., -0.5726, -0.5566, -0.4958],\n",
       "          [-0.0174, -0.0346,  0.0222,  ...,  0.0207,  0.0106, -0.0453]]]),\n",
       " tensor([[[-0.9058, -0.9778, -0.6808,  ..., -0.4224,  0.2937,  0.6470],\n",
       "          [-0.2821,  0.1633, -0.0692,  ..., -0.8020,  0.0366, -0.8279],\n",
       "          [ 0.0033, -0.0395, -0.0051,  ...,  0.0044, -0.0143, -0.0405]]]),\n",
       " tensor([[[-1.4268, -0.5541, -1.1470,  ..., -0.0106,  0.1230,  1.2595],\n",
       "          [-0.6685, -0.4801, -1.0215,  ..., -0.4477,  0.2517, -0.2407],\n",
       "          [-0.0281, -0.0652, -0.0086,  ..., -0.0074,  0.0132, -0.0600]]]),\n",
       " tensor([[[-1.5504, -0.5364, -1.2643,  ..., -0.6518, -0.0025,  1.0714],\n",
       "          [-0.8516, -0.2341, -0.7218,  ..., -0.4597, -0.1297, -0.4168],\n",
       "          [-0.0183, -0.0542,  0.0022,  ..., -0.0496, -0.0302, -0.0870]]]),\n",
       " tensor([[[-1.7478, -0.7176, -1.4446,  ..., -0.0459,  0.3797,  0.8772],\n",
       "          [-1.2437, -0.1891, -0.6820,  ..., -0.3134,  0.2850, -0.4985],\n",
       "          [-0.0146, -0.0219,  0.0043,  ..., -0.0803, -0.0641, -0.0191]]]),\n",
       " tensor([[[-1.6239, -0.7750, -1.1219,  ..., -0.1473,  0.2898,  0.7232],\n",
       "          [-1.0256, -0.3892, -0.4098,  ..., -0.3498,  0.3374, -0.3997],\n",
       "          [ 0.0065, -0.0063, -0.1165,  ...,  0.0635, -0.0688, -0.0159]]]),\n",
       " tensor([[[-1.3619, -0.6809, -0.5387,  ..., -0.1404,  0.1658,  0.6824],\n",
       "          [-1.0779, -0.8755,  0.0053,  ..., -0.2874,  0.5010, -0.1286],\n",
       "          [ 0.0595,  0.0124, -0.0260,  ...,  0.0135, -0.0466,  0.0223]]]),\n",
       " tensor([[[-0.4422, -0.0526, -0.0091,  ...,  0.0963,  0.1625,  0.3031],\n",
       "          [-0.4922, -0.6787,  0.0211,  ...,  0.2248,  0.6457, -0.2831],\n",
       "          [ 1.0238,  0.0883, -0.3741,  ...,  0.3148, -0.8268, -0.2171]]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da52802",
   "metadata": {},
   "source": [
    "## Creating the concatenation Vector and the Sum Vector for every word in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "extraordinary-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_vec = []\n",
    "# sum_vec = []\n",
    "# for index, row in df_vocab.iterrows():\n",
    "#     text = row[\"Word\"]\n",
    "#     marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    \n",
    "#     tokenized_text = tokenizer.tokenize(marked_text)\n",
    "#     indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "#     segments_ids = [1] * len(tokenized_text)\n",
    "    \n",
    "#     tokens_tensor = torch.tensor([indexed_tokens])\n",
    "#     segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "\n",
    "#         outputs = model(tokens_tensor, segments_tensors)  \n",
    "#         hidden_states = outputs[2]\n",
    "        \n",
    "#         token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "        \n",
    "#         token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        \n",
    "#         token_embeddings = token_embeddings.permute(1,0,2)\n",
    "        \n",
    "#     token_vecs_cat = [torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0) for token in token_embeddings]\n",
    "    \n",
    "#     token_vecs_sum = [torch.sum(token[-4:], dim=0) for token in token_embeddings]\n",
    "    \n",
    "#     main_cat_vec = token_vecs_cat[-2]\n",
    "    \n",
    "#     main_sum_vec = token_vecs_sum[-2]\n",
    "    \n",
    "#     cat_vec.append(main_cat_vec)\n",
    "#     sum_vec.append(main_sum_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc255c",
   "metadata": {},
   "source": [
    "## Saving the concatenation and sum vectors as pkl file for easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "decimal-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# file_name_1 = \"cat_vec.pkl\"\n",
    "# file_name_2 = \"sum_vec.pkl\"\n",
    "# open_file_1 = open(file_name_1, \"wb\")\n",
    "# open_file_2 = open(file_name_2, \"wb\")\n",
    "# pickle.dump(cat_vec, open_file_1)\n",
    "# pickle.dump(sum_vec, open_file_2)\n",
    "# open_file_1.close()\n",
    "# open_file_2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b573bb",
   "metadata": {},
   "source": [
    "## Loading the saved pkl files of sum and concatenation vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "olympic-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f1 = open(\"cat_vec.pkl\",\"rb\")\n",
    "f2 = open(\"sum_vec.pkl\",\"rb\")\n",
    "cat_vec  = pickle.load(f1)\n",
    "sum_vec = pickle.load(f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fca2738",
   "metadata": {},
   "source": [
    "## Task 1: Create a function to compute cosine similarity between any two word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ffb0f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(word1, word2, vec_type):\n",
    "    return 1 - cosine(vec_type[list(df_vocab[\"Word\"]).index(word1)], vec_type[list(df_vocab[\"Word\"]).index(word2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652798a5",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67316474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816037654876709\n",
      "0.8221496939659119\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(\"brain\",\"skull\",cat_vec))\n",
    "print(cosine_similarity(\"brain\",\"skull\",sum_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ff4f2",
   "metadata": {},
   "source": [
    "## Task 2: Create a function to find the word closest to another word in the vocab using cosines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60a5b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(word, vec_type):\n",
    "  sims = [cosine_similarity(word,x,vec_type) for x in list(df_vocab[\"Word\"])[:len(vec_type)]]\n",
    "  y = np.array(sims)\n",
    "  y_sorted = np.argsort(-y) ## gives sorted indices\n",
    "  top20_indices = y_sorted[:5]\n",
    "  w1 = [list(df_vocab[\"Word\"])[i] for i in top20_indices]\n",
    "  return w1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9da0e",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69b5d687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brain', 'brains', 'skull', 'consciousness', 'liver']\n",
      "['cat', 'kitten', 'cats', 'squirrel', 'doll']\n",
      "['apple', 'banana', 'tomato', 'iPad', 'chocolate']\n",
      "['exam', 'examination', 'exams', 'quiz', 'degree']\n",
      "['brain', 'brains', 'skull', 'consciousness', 'liver']\n",
      "['cat', 'kitten', 'cats', 'squirrel', 'rat']\n",
      "['apple', 'banana', 'tomato', 'iPad', 'chocolate']\n",
      "['exam', 'examination', 'exams', 'quiz', 'degree']\n"
     ]
    }
   ],
   "source": [
    "print(find_closest('brain', cat_vec))\n",
    "print(find_closest('cat', cat_vec))\n",
    "print(find_closest('apple', cat_vec))\n",
    "print(find_closest('exam', cat_vec))\n",
    "\n",
    "print(find_closest('brain', sum_vec))\n",
    "print(find_closest('cat', sum_vec))\n",
    "print(find_closest('apple', sum_vec))\n",
    "print(find_closest('exam', sum_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02c0718",
   "metadata": {},
   "source": [
    "## Task 3: Create a function to compute the average vector of any two vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cbb8b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_vector(word1, word2, vec_type):\n",
    "    return torch.divide(torch.add(vec_type[list(df_vocab[\"Word\"]).index(word1)],vec_type[list(df_vocab[\"Word\"]).index(word2)]),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2568a2ba",
   "metadata": {},
   "source": [
    "## Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "03daf6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3072])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "print(avg_vector(\"brain\",\"skull\",cat_vec).size())\n",
    "print(avg_vector(\"brain\",\"skull\",sum_vec).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f05c7",
   "metadata": {},
   "source": [
    "## Task 4: Create a function to find the word in the vocab closest to an average vector for each wordpair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01e904f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_word(word1, word2, vec_type):\n",
    "    avg_vector_tensor = avg_vector(word1, word2, vec_type)\n",
    "    sims_cosine = [1 - cosine(avg_vector_tensor,vec_type[list(df_vocab[\"Word\"]).index(x)]) for x in list(df_vocab[\"Word\"])[:len(vec_type)]]\n",
    "    y = np.array(sims_cosine)\n",
    "    y_sorted = np.argsort(-y) ## gives sorted indices\n",
    "    top20_indices = y_sorted[:5]\n",
    "    w1 = [list(df_vocab[\"Word\"])[i] for i in top20_indices]\n",
    "    return w1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eed7fb",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "16913c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lion', 'tiger', 'monkey', 'panther', 'squirrel']\n",
      "['lion', 'tiger', 'monkey', 'panther', 'leopard']\n"
     ]
    }
   ],
   "source": [
    "print(closest_word(\"lion\",\"tiger\",cat_vec))\n",
    "print(closest_word(\"lion\",\"tiger\",sum_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5fd05a",
   "metadata": {},
   "source": [
    "## Testing function on a csv file with pair of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "round-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_pair = pd.read_csv(\"connector_wordpairs_boards.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "received-egyptian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word1</th>\n",
       "      <th>Word2</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>boardnames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>void</td>\n",
       "      <td>couch</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board1_words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>giggle</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board1_words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exam</td>\n",
       "      <td>algebra</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board1_words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tea</td>\n",
       "      <td>bean</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board10_words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tourist</td>\n",
       "      <td>comedy</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board10_words</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word1     Word2 Experiment        boardnames\n",
       "0     void     couch         E1   e1_board1_words\n",
       "1   giggle  abnormal         E1   e1_board1_words\n",
       "2     exam   algebra         E1   e1_board1_words\n",
       "3      tea      bean         E1  e1_board10_words\n",
       "4  tourist    comedy         E1  e1_board10_words"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_pair.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "061c9f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_word_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9dd3c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_similarity = []\n",
    "sum_similarity = []\n",
    "for index, row in df_word_pair.iterrows():\n",
    "    word1 = row[\"Word1\"]\n",
    "    word2 = row[\"Word2\"]\n",
    "    cat_similarity.append(closest_word(word1, word2, cat_vec)[2:])\n",
    "    sum_similarity.append(closest_word(word1, word2, sum_vec)[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d91ee592",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_pair[\"top 3 similar words(sum)\"] = sum_similarity\n",
    "df_word_pair[\"top 3 similar words(cat)\"] = cat_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a676831",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8d6073cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word1</th>\n",
       "      <th>Word2</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>boardnames</th>\n",
       "      <th>top 3 similar words(sum)</th>\n",
       "      <th>top 3 similar words(cat)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>void</td>\n",
       "      <td>couch</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board1_words</td>\n",
       "      <td>[sofa, tile, furniture]</td>\n",
       "      <td>[sofa, tile, furniture]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>giggle</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board1_words</td>\n",
       "      <td>[confuse, giggle, clown]</td>\n",
       "      <td>[giggle, confuse, clown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exam</td>\n",
       "      <td>algebra</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board1_words</td>\n",
       "      <td>[examination, exams, analysis]</td>\n",
       "      <td>[examination, analysis, calculus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tea</td>\n",
       "      <td>bean</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board10_words</td>\n",
       "      <td>[spaghetti, stink, doll]</td>\n",
       "      <td>[spaghetti, doll, stink]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tourist</td>\n",
       "      <td>comedy</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board10_words</td>\n",
       "      <td>[sitcom, tourists, nursing]</td>\n",
       "      <td>[sitcom, tourists, nursing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pendulum</td>\n",
       "      <td>dusk</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board10_words</td>\n",
       "      <td>[midnight, sunrise, handcuffs]</td>\n",
       "      <td>[midnight, sunrise, handcuffs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>beginning</td>\n",
       "      <td>brake</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board2_words</td>\n",
       "      <td>[steer, fireplace, harness]</td>\n",
       "      <td>[steer, harness, fireplace]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>birds</td>\n",
       "      <td>aircraft</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board2_words</td>\n",
       "      <td>[animals, planes, vehicles]</td>\n",
       "      <td>[animals, planes, vehicles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>school</td>\n",
       "      <td>stop</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board2_words</td>\n",
       "      <td>[garage, stopping, trouble]</td>\n",
       "      <td>[garage, stopping, dodge]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>circle</td>\n",
       "      <td>dance</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board3_words</td>\n",
       "      <td>[jump, rhythm, jumping]</td>\n",
       "      <td>[jump, jumping, rhythm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>famine</td>\n",
       "      <td>calorie</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board3_words</td>\n",
       "      <td>[starvation, junk food, poverty]</td>\n",
       "      <td>[starvation, junk food, poverty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hundred</td>\n",
       "      <td>economy</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board3_words</td>\n",
       "      <td>[prosperity, thousand, dozen]</td>\n",
       "      <td>[thousand, prosperity, dozen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>corpse</td>\n",
       "      <td>fight</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board4_words</td>\n",
       "      <td>[dying, fighting, stump]</td>\n",
       "      <td>[dying, fighting, warehouse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>graph</td>\n",
       "      <td>copy</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board4_words</td>\n",
       "      <td>[diagram, document, image]</td>\n",
       "      <td>[diagram, document, image]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>egg</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board4_words</td>\n",
       "      <td>[oven, garage, backyard]</td>\n",
       "      <td>[oven, garage, pantry]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>near</td>\n",
       "      <td>astronaut</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board5_words</td>\n",
       "      <td>[superhero, alien, scientist]</td>\n",
       "      <td>[superhero, scientist, alien]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>homeless</td>\n",
       "      <td>apartment</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board5_words</td>\n",
       "      <td>[elderly, apartments, housing]</td>\n",
       "      <td>[elderly, apartments, housing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>onion</td>\n",
       "      <td>cigarette</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board5_words</td>\n",
       "      <td>[sulfur, popcorn, umbrella]</td>\n",
       "      <td>[sulfur, popcorn, umbrella]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pants</td>\n",
       "      <td>collar</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board6_words</td>\n",
       "      <td>[denim, scarf, pajamas]</td>\n",
       "      <td>[denim, scarf, pajamas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>definition</td>\n",
       "      <td>assist</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board6_words</td>\n",
       "      <td>[transfer, request, manage]</td>\n",
       "      <td>[transfer, request, update]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>war</td>\n",
       "      <td>quiet</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board6_words</td>\n",
       "      <td>[peace, conversation, silent]</td>\n",
       "      <td>[peace, conversation, battle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>flat</td>\n",
       "      <td>alike</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board7_words</td>\n",
       "      <td>[balanced, uneven, swelling]</td>\n",
       "      <td>[balanced, uneven, swelling]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>perjury</td>\n",
       "      <td>adultery</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board7_words</td>\n",
       "      <td>[rape, faulty, insult]</td>\n",
       "      <td>[rape, faulty, murder]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>heavy</td>\n",
       "      <td>feather</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board7_words</td>\n",
       "      <td>[massive, strict, bulky]</td>\n",
       "      <td>[strict, massive, binding]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>breeze</td>\n",
       "      <td>bubble</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board8_words</td>\n",
       "      <td>[bubbles, oven, blow]</td>\n",
       "      <td>[bubbles, oven, blow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bad</td>\n",
       "      <td>actress</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board8_words</td>\n",
       "      <td>[actor, actors, shitty]</td>\n",
       "      <td>[actor, actors, shitty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>communicate</td>\n",
       "      <td>cooking</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board8_words</td>\n",
       "      <td>[connect, talking, healing]</td>\n",
       "      <td>[connect, talking, healing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bat</td>\n",
       "      <td>bounce</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board9_words</td>\n",
       "      <td>[glove, blow, chick]</td>\n",
       "      <td>[glove, blow, garage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>toes</td>\n",
       "      <td>Dracula</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board9_words</td>\n",
       "      <td>[toe, Cinderella, barefoot]</td>\n",
       "      <td>[toe, Cinderella, barefoot]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lunch</td>\n",
       "      <td>almond</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board9_words</td>\n",
       "      <td>[breakfast, popcorn, oyster]</td>\n",
       "      <td>[breakfast, dinner, popcorn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>cave</td>\n",
       "      <td>knight</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board1_words</td>\n",
       "      <td>[tunnel, cottage, turtle]</td>\n",
       "      <td>[tunnel, cottage, mansion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>quick</td>\n",
       "      <td>glow</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board1_words</td>\n",
       "      <td>[flash, flicker, glowing]</td>\n",
       "      <td>[flash, glowing, flicker]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>tree</td>\n",
       "      <td>oak</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board1_words</td>\n",
       "      <td>[branches, foliage, squirrel]</td>\n",
       "      <td>[branches, squirrel, foliage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>garage</td>\n",
       "      <td>bone</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board10_words</td>\n",
       "      <td>[fatigue, warehouse, stump]</td>\n",
       "      <td>[warehouse, fatigue, stump]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>feet</td>\n",
       "      <td>chapel</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board10_words</td>\n",
       "      <td>[sandals, shoes, cottage]</td>\n",
       "      <td>[sandals, shoes, cottage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>chair</td>\n",
       "      <td>table</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board10_words</td>\n",
       "      <td>[tables, stool, sofa]</td>\n",
       "      <td>[tables, stool, sofa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>stern</td>\n",
       "      <td>wind</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board2_words</td>\n",
       "      <td>[gust, sails, surf]</td>\n",
       "      <td>[gust, sails, surf]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>sun</td>\n",
       "      <td>bowl</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board2_words</td>\n",
       "      <td>[dome, bowls, dish]</td>\n",
       "      <td>[dome, bowls, dish]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>gold</td>\n",
       "      <td>silver</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board2_words</td>\n",
       "      <td>[bronze, yellow, golden]</td>\n",
       "      <td>[yellow, golden, bronze]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>holy</td>\n",
       "      <td>kind</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board3_words</td>\n",
       "      <td>[massive, strict, pierced]</td>\n",
       "      <td>[massive, pierced, strict]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>teeth</td>\n",
       "      <td>gums</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board3_words</td>\n",
       "      <td>[buns, rubbers, frosting]</td>\n",
       "      <td>[rubbers, buns, oodles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>rude</td>\n",
       "      <td>regret</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board3_words</td>\n",
       "      <td>[humiliation, shame, irritation]</td>\n",
       "      <td>[humiliation, shame, irritation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>dream</td>\n",
       "      <td>bet</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board4_words</td>\n",
       "      <td>[dreaming, nightmare, snack]</td>\n",
       "      <td>[dreaming, nightmare, blow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>jump</td>\n",
       "      <td>leap</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board4_words</td>\n",
       "      <td>[jumping, bounce, freeze]</td>\n",
       "      <td>[jumping, bounce, freeze]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>travel</td>\n",
       "      <td>ankle</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board4_words</td>\n",
       "      <td>[traveling, wheelchair, massage]</td>\n",
       "      <td>[wheelchair, traveling, camping]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>snake</td>\n",
       "      <td>ash</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board5_words</td>\n",
       "      <td>[kitten, lizard, squirrel]</td>\n",
       "      <td>[lizard, kitten, squirrel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>hand</td>\n",
       "      <td>birth</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board5_words</td>\n",
       "      <td>[hands, fingers, cheek]</td>\n",
       "      <td>[hands, fingers, name]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>lion</td>\n",
       "      <td>tiger</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board5_words</td>\n",
       "      <td>[monkey, panther, leopard]</td>\n",
       "      <td>[monkey, panther, squirrel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>elm</td>\n",
       "      <td>rock</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board6_words</td>\n",
       "      <td>[oak, cedar, leaf]</td>\n",
       "      <td>[oak, cedar, leaf]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>crust</td>\n",
       "      <td>boot</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board6_words</td>\n",
       "      <td>[glove, oven, nail]</td>\n",
       "      <td>[glove, oven, nail]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>sit</td>\n",
       "      <td>stand</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board6_words</td>\n",
       "      <td>[eat, stay, sitting]</td>\n",
       "      <td>[eat, stay, sitting]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>east</td>\n",
       "      <td>short</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board7_words</td>\n",
       "      <td>[north, south, straight]</td>\n",
       "      <td>[north, south, straight]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>old</td>\n",
       "      <td>new</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board7_words</td>\n",
       "      <td>[outdated, antique, kid]</td>\n",
       "      <td>[outdated, antique, kid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>glass</td>\n",
       "      <td>cage</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board7_words</td>\n",
       "      <td>[stone, mason, kitten]</td>\n",
       "      <td>[stone, mason, kitten]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>giant</td>\n",
       "      <td>subtle</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board8_words</td>\n",
       "      <td>[bulky, smoky, shaky]</td>\n",
       "      <td>[smoky, shaky, bulky]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>army</td>\n",
       "      <td>drum</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board8_words</td>\n",
       "      <td>[military, mortar, soldier]</td>\n",
       "      <td>[mortar, military, housing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>happy</td>\n",
       "      <td>sad</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board8_words</td>\n",
       "      <td>[sadness, happiness, pleased]</td>\n",
       "      <td>[sadness, happiness, pleased]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>weird</td>\n",
       "      <td>trauma</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board9_words</td>\n",
       "      <td>[strange, scary, confusing]</td>\n",
       "      <td>[strange, confusing, scary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>olive</td>\n",
       "      <td>real</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board9_words</td>\n",
       "      <td>[lemon, Thanksgiving, purple]</td>\n",
       "      <td>[lemon, purple, Thanksgiving]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>candle</td>\n",
       "      <td>wick</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board9_words</td>\n",
       "      <td>[spark, cookie, flicker]</td>\n",
       "      <td>[spark, cookie, flicker]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word1      Word2 Experiment        boardnames  \\\n",
       "0          void      couch         E1   e1_board1_words   \n",
       "1        giggle   abnormal         E1   e1_board1_words   \n",
       "2          exam    algebra         E1   e1_board1_words   \n",
       "3           tea       bean         E1  e1_board10_words   \n",
       "4       tourist     comedy         E1  e1_board10_words   \n",
       "5      pendulum       dusk         E1  e1_board10_words   \n",
       "6     beginning      brake         E1   e1_board2_words   \n",
       "7         birds   aircraft         E1   e1_board2_words   \n",
       "8        school       stop         E1   e1_board2_words   \n",
       "9        circle      dance         E1   e1_board3_words   \n",
       "10       famine    calorie         E1   e1_board3_words   \n",
       "11      hundred    economy         E1   e1_board3_words   \n",
       "12       corpse      fight         E1   e1_board4_words   \n",
       "13        graph       copy         E1   e1_board4_words   \n",
       "14      kitchen        egg         E1   e1_board4_words   \n",
       "15         near  astronaut         E1   e1_board5_words   \n",
       "16     homeless  apartment         E1   e1_board5_words   \n",
       "17        onion  cigarette         E1   e1_board5_words   \n",
       "18        pants     collar         E1   e1_board6_words   \n",
       "19   definition     assist         E1   e1_board6_words   \n",
       "20          war      quiet         E1   e1_board6_words   \n",
       "21         flat      alike         E1   e1_board7_words   \n",
       "22      perjury   adultery         E1   e1_board7_words   \n",
       "23        heavy    feather         E1   e1_board7_words   \n",
       "24       breeze     bubble         E1   e1_board8_words   \n",
       "25          bad    actress         E1   e1_board8_words   \n",
       "26  communicate    cooking         E1   e1_board8_words   \n",
       "27          bat     bounce         E1   e1_board9_words   \n",
       "28         toes    Dracula         E1   e1_board9_words   \n",
       "29        lunch     almond         E1   e1_board9_words   \n",
       "30         cave     knight         E2   e2_board1_words   \n",
       "31        quick       glow         E2   e2_board1_words   \n",
       "32         tree        oak         E2   e2_board1_words   \n",
       "33       garage       bone         E2  e2_board10_words   \n",
       "34         feet     chapel         E2  e2_board10_words   \n",
       "35        chair      table         E2  e2_board10_words   \n",
       "36        stern       wind         E2   e2_board2_words   \n",
       "37          sun       bowl         E2   e2_board2_words   \n",
       "38         gold     silver         E2   e2_board2_words   \n",
       "39         holy       kind         E2   e2_board3_words   \n",
       "40        teeth       gums         E2   e2_board3_words   \n",
       "41         rude     regret         E2   e2_board3_words   \n",
       "42        dream        bet         E2   e2_board4_words   \n",
       "43         jump       leap         E2   e2_board4_words   \n",
       "44       travel      ankle         E2   e2_board4_words   \n",
       "45        snake        ash         E2   e2_board5_words   \n",
       "46         hand      birth         E2   e2_board5_words   \n",
       "47         lion      tiger         E2   e2_board5_words   \n",
       "48          elm       rock         E2   e2_board6_words   \n",
       "49        crust       boot         E2   e2_board6_words   \n",
       "50          sit      stand         E2   e2_board6_words   \n",
       "51         east      short         E2   e2_board7_words   \n",
       "52          old        new         E2   e2_board7_words   \n",
       "53        glass       cage         E2   e2_board7_words   \n",
       "54        giant     subtle         E2   e2_board8_words   \n",
       "55         army       drum         E2   e2_board8_words   \n",
       "56        happy        sad         E2   e2_board8_words   \n",
       "57        weird     trauma         E2   e2_board9_words   \n",
       "58        olive       real         E2   e2_board9_words   \n",
       "59       candle       wick         E2   e2_board9_words   \n",
       "\n",
       "            top 3 similar words(sum)           top 3 similar words(cat)  \n",
       "0            [sofa, tile, furniture]            [sofa, tile, furniture]  \n",
       "1           [confuse, giggle, clown]           [giggle, confuse, clown]  \n",
       "2     [examination, exams, analysis]  [examination, analysis, calculus]  \n",
       "3           [spaghetti, stink, doll]           [spaghetti, doll, stink]  \n",
       "4        [sitcom, tourists, nursing]        [sitcom, tourists, nursing]  \n",
       "5     [midnight, sunrise, handcuffs]     [midnight, sunrise, handcuffs]  \n",
       "6        [steer, fireplace, harness]        [steer, harness, fireplace]  \n",
       "7        [animals, planes, vehicles]        [animals, planes, vehicles]  \n",
       "8        [garage, stopping, trouble]          [garage, stopping, dodge]  \n",
       "9            [jump, rhythm, jumping]            [jump, jumping, rhythm]  \n",
       "10  [starvation, junk food, poverty]   [starvation, junk food, poverty]  \n",
       "11     [prosperity, thousand, dozen]      [thousand, prosperity, dozen]  \n",
       "12          [dying, fighting, stump]       [dying, fighting, warehouse]  \n",
       "13        [diagram, document, image]         [diagram, document, image]  \n",
       "14          [oven, garage, backyard]             [oven, garage, pantry]  \n",
       "15     [superhero, alien, scientist]      [superhero, scientist, alien]  \n",
       "16    [elderly, apartments, housing]     [elderly, apartments, housing]  \n",
       "17       [sulfur, popcorn, umbrella]        [sulfur, popcorn, umbrella]  \n",
       "18           [denim, scarf, pajamas]            [denim, scarf, pajamas]  \n",
       "19       [transfer, request, manage]        [transfer, request, update]  \n",
       "20     [peace, conversation, silent]      [peace, conversation, battle]  \n",
       "21      [balanced, uneven, swelling]       [balanced, uneven, swelling]  \n",
       "22            [rape, faulty, insult]             [rape, faulty, murder]  \n",
       "23          [massive, strict, bulky]         [strict, massive, binding]  \n",
       "24             [bubbles, oven, blow]              [bubbles, oven, blow]  \n",
       "25           [actor, actors, shitty]            [actor, actors, shitty]  \n",
       "26       [connect, talking, healing]        [connect, talking, healing]  \n",
       "27              [glove, blow, chick]              [glove, blow, garage]  \n",
       "28       [toe, Cinderella, barefoot]        [toe, Cinderella, barefoot]  \n",
       "29      [breakfast, popcorn, oyster]       [breakfast, dinner, popcorn]  \n",
       "30         [tunnel, cottage, turtle]         [tunnel, cottage, mansion]  \n",
       "31         [flash, flicker, glowing]          [flash, glowing, flicker]  \n",
       "32     [branches, foliage, squirrel]      [branches, squirrel, foliage]  \n",
       "33       [fatigue, warehouse, stump]        [warehouse, fatigue, stump]  \n",
       "34         [sandals, shoes, cottage]          [sandals, shoes, cottage]  \n",
       "35             [tables, stool, sofa]              [tables, stool, sofa]  \n",
       "36               [gust, sails, surf]                [gust, sails, surf]  \n",
       "37               [dome, bowls, dish]                [dome, bowls, dish]  \n",
       "38          [bronze, yellow, golden]           [yellow, golden, bronze]  \n",
       "39        [massive, strict, pierced]         [massive, pierced, strict]  \n",
       "40         [buns, rubbers, frosting]            [rubbers, buns, oodles]  \n",
       "41  [humiliation, shame, irritation]   [humiliation, shame, irritation]  \n",
       "42      [dreaming, nightmare, snack]        [dreaming, nightmare, blow]  \n",
       "43         [jumping, bounce, freeze]          [jumping, bounce, freeze]  \n",
       "44  [traveling, wheelchair, massage]   [wheelchair, traveling, camping]  \n",
       "45        [kitten, lizard, squirrel]         [lizard, kitten, squirrel]  \n",
       "46           [hands, fingers, cheek]             [hands, fingers, name]  \n",
       "47        [monkey, panther, leopard]        [monkey, panther, squirrel]  \n",
       "48                [oak, cedar, leaf]                 [oak, cedar, leaf]  \n",
       "49               [glove, oven, nail]                [glove, oven, nail]  \n",
       "50              [eat, stay, sitting]               [eat, stay, sitting]  \n",
       "51          [north, south, straight]           [north, south, straight]  \n",
       "52          [outdated, antique, kid]           [outdated, antique, kid]  \n",
       "53            [stone, mason, kitten]             [stone, mason, kitten]  \n",
       "54             [bulky, smoky, shaky]              [smoky, shaky, bulky]  \n",
       "55       [military, mortar, soldier]        [mortar, military, housing]  \n",
       "56     [sadness, happiness, pleased]      [sadness, happiness, pleased]  \n",
       "57       [strange, scary, confusing]        [strange, confusing, scary]  \n",
       "58     [lemon, Thanksgiving, purple]      [lemon, purple, Thanksgiving]  \n",
       "59          [spark, cookie, flicker]           [spark, cookie, flicker]  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_pair.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1e8d7a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_pair.to_csv(\"word_pair_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
