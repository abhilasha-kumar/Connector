---
title: "Connector-RSA"
output: pdf_document
---

# Imports

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidyboot)
library(here)
library(ggthemes)
library(GGally)
library(lme4)
library(lmerTest)
```

clean up empirical data from the connector games.

```{r}
c <- read_csv(here("./data/raw_data.csv")) %>%
  filter(!is.na(P2W1)) %>%
  unite(wordpair, Word1, Word2, sep = '-') %>%
  unite(chosen_wordpair1, P2W1, P2W2, sep = '-', remove = F) %>%
  unite(chosen_wordpair2, P2W2, P2W1, sep = '-', remove = T) %>%
  select(Subject, Experiment, wordpair, Trial, Clue1, chosen_wordpair1, chosen_wordpair2) %>%
  rename(true_wordpair = wordpair)

c_clues = read_csv(here("./data/final_board_clues_all.csv")) %>% 
  mutate(wordpair = gsub(' - ', '-', wordpair)) %>%
  distinct()
```

# Behavioral results

We import the csv generated by the `descriptive_analyses.ipynb` notebook.

```{r}
c.sims = read_csv(here("../data/descriptive_precomputed.csv")) %>%
  unite("wordpair", Word1, Word2, sep = '-') %>%
  pivot_longer(names_to = 'comparison', values_to = 'sim', cols = `bert-sum_w1w2_sim`:`swow_c2_c1avg_sim`) %>%
  separate(comparison, sep='_', into = c('representation', 'comparison'), extra = 'merge') %>%
  mutate(representation = fct_recode(representation, BERT = "bert-sum", GloVe = "glove", SWOW = "swow"),
         representation = fct_relevel(representation, "GloVe", "BERT", "SWOW")) 
```

## Fig 2: Clue 1 

First we look at whether Clue1 is close to W1, W2, or the midpoint (W1+W2/2)

```{r}
clue.dist <- c.sims %>%
  group_by(Subject, representation, comparison) %>%
  summarise(sim = mean(sim, na.rm = TRUE)) %>%
  pivot_wider(names_from = comparison, values_from = sim) %>%
  transmute(baseline_c1 = min(c1w1_sim, c1w2_sim), 
         word1_c1 = min(c1w1_sim, c1w2_sim) - baseline_c1, 
         word2_c1 = max(c1w1_sim, c1w2_sim) - baseline_c1,
         midpoint_c1 = c1avg_sim - baseline_c1,
         baseline_c2 = min(c2w1_sim, c2w2_sim),
         word1_c2 = min(c2w1_sim, c2w2_sim) - baseline_c2,
         word2_c2 = max(c2w1_sim, c2w2_sim) - baseline_c2,
         midpoint_c2 = c2_w1w2avg_sim - baseline_c2,
         centroid_c2 = c2_c1avg_sim - baseline_c2) %>%
  pivot_longer(cols = -c(Subject, representation), names_to = 'comparison') %>%
  separate(comparison, into = c('comparison', 'clue'))

clue.dist %>%
  group_by(representation, comparison, clue) %>%
  tidyboot_mean(value) %>%
  filter(comparison != 'baseline') %>%
  ungroup() %>%
  mutate(comparison = fct_relevel(comparison, "word1", "word2", "midpoint", "centroid")) %>%
  ggplot(aes(x = comparison, y = empirical_stat, color = representation, group = representation)) + 
    geom_errorbar(aes(ymin=ci_lower, ymax=ci_upper), size = 0.5, width=0, 
                  color = "gray26", position = position_dodge(0))+
    geom_point(size = 2) + 
    geom_line(size = 1)+
    geom_hline(yintercept = 0, linetype = 'dotted') +
    scale_color_solarized() +
    labs(x = "", y = "cosine similarity") +
    facet_wrap(~ clue) +
    theme_few()+
    theme(aspect.ratio = 1)

ggsave('combined_similarities.pdf', width = 5, height = 4, units = 'in')
```

## Item Level

### Figure 2

```{r}
clue1_plotdata = c.sims %>%
  group_by(representation, wordpair, Clue1, comparison) %>%
summarise(sim = mean(sim, na.rm = TRUE)) %>%
  mutate(sim = ifelse(is.nan(sim), NA, sim)) %>%
  pivot_wider(names_from = comparison, values_from = sim)  %>%
  transmute(baseline_c1 = min(c1w1_sim,c1w2_sim), 
         farther_c1 = min(c1w1_sim, c1w2_sim) - baseline_c1, 
         closer_c1 = max(c1w1_sim, c1w2_sim) - baseline_c1,
         midpoint_c1 = c1avg_sim - baseline_c1) %>%
  pivot_longer(cols = -c(wordpair, representation, Clue1), names_to = 'comparison') %>%
  separate(comparison, into = c('comparison', 'clue')) %>% 
  group_by(representation, comparison, clue) %>%
  tidyboot_mean(value, na.rm = TRUE) %>%
  filter(comparison != 'baseline') %>%
  ungroup() %>%
  mutate(comparison = fct_relevel(comparison, "farther", "closer", "midpoint")) 

clue2_plotdata = c.sims %>%
  group_by(representation, wordpair, Clue1, Clue2, comparison) %>%
summarise(sim = mean(sim, na.rm = TRUE)) %>%
  mutate(sim = ifelse(is.nan(sim), NA, sim)) %>%
  pivot_wider(names_from = comparison, values_from = sim)  %>%
  transmute(baseline_c2 = min(c2w1_sim, c2w2_sim),
         farther_c2 = min(c2w1_sim, c2w2_sim) - baseline_c2,
         closer_c2 = max(c2w1_sim, c2w2_sim) - baseline_c2,
         midpoint_c2 = c2_w1w2avg_sim - baseline_c2,
         centroid_c2 = c2_c1avg_sim - baseline_c2) %>%
  pivot_longer(cols = -c(wordpair, representation, Clue1, Clue2), names_to = 'comparison') %>%
  separate(comparison, into = c('comparison', 'clue')) %>% 
  group_by(representation, comparison, clue) %>%
  tidyboot_mean(value, na.rm = TRUE) %>%
  filter(comparison != 'baseline') %>%
  ungroup() %>%
  mutate(comparison = fct_relevel(comparison, "farther", "closer", "midpoint", "centroid")) 

rbind(clue1_plotdata, clue2_plotdata) %>%
  ggplot(aes(x = comparison, y = empirical_stat, color = representation, group = representation)) + 
    geom_errorbar(aes(ymin=ci_lower, ymax=ci_upper), size = 0.5, width=0, 
                  color = "gray26", position = position_dodge(0))+
    geom_point(size = 2) + 
    geom_line(size = 1)+
    geom_hline(yintercept = 0, linetype = 'dotted') +
    scale_color_solarized() +
    labs(x = "", y = "cosine similarity") +
    facet_wrap(~ clue) +
    theme_few()+
    theme(aspect.ratio = 1)

```


### lmer: clue 1

```{r}
clue1_lmer_model = c.sims %>%
  group_by(representation, wordpair, Clue1, comparison) %>%
pivot_wider(names_from = comparison, values_from = sim) %>%
   group_by(representation, wordpair, Clue1) %>%
   transmute(c1w1 = min(c1w1_sim, c1w2_sim),
             c1w2 = max(c1w1_sim, c1w2_sim),
             midpoint = c1avg_sim) %>%
   pivot_longer(cols = -c(wordpair, Clue1, representation), names_to = 'comparison') %>%
  nest(-representation) %>%
  mutate(results = map(data, ~ {
     lmer(value ~ comparison + (1 | wordpair),
          contrasts = list(comparison = contr.treatment(3, base = 2)),
          data = .) %>%
      broom.mixed::tidy()
    })) %>%
  unnest(results)

```

### lmer: clue 2

```{r}
clue2_lmer_model = c.sims %>%
  group_by(representation, wordpair, Clue1, Clue2, comparison) %>%
pivot_wider(names_from = comparison, values_from = sim) %>%
   group_by(representation, wordpair, Clue1, Clue2) %>%
   transmute(c1w1 = min(c2w1_sim, c2w2_sim),
             c1w2 = max(c2w1_sim, c2w2_sim),
             midpoint = c2_w1w2avg_sim,
             centroid = c2_c1avg_sim) %>%
   pivot_longer(cols = -c(wordpair, Clue1, Clue2, representation), names_to = 'comparison') %>%
  nest(-representation) %>%
  mutate(results = map(data, ~ {
     lmer(value ~ comparison + (1 | wordpair),
          contrasts = list(comparison = contr.treatment(4, base = 2)),
          data = .) %>%
      broom.mixed::tidy()
    })) %>%
  unnest(results)

```


## Statistical tests

### Clue 1

Trial-level regression

```{r}
c.sims %>%
  group_by(Subject, Trial, Level, representation, comparison) %>%
  pivot_wider(names_from = comparison, values_from = sim) %>%
  group_by(representation, Trial, Level, Subject) %>%
  transmute(tocloser_clue1 = max(c1w1_sim, c1w2_sim) - min(c1w1_sim, c1w2_sim),
            tomidpoint_clue1 = c1avg_sim - min(c1w1_sim, c1w2_sim),
            tocloser_clue2 = max(c2w1_sim, c2w2_sim) - min(c2w1_sim, c2w2_sim),
            tomidpoint_clue2 = c2_w1w2avg_sim - min(c2w1_sim, c2w2_sim)) %>%
  pivot_longer(cols = -c(Subject, Trial, Level, representation), names_to = 'comparison') %>%
  separate(comparison, into = c('comparison', 'clue')) %>%
  ungroup() %>%
  mutate(comparison = fct_relevel(factor(comparison), 'closer')) %>%
  ggplot(aes(x = Level, y = value, color = comparison)) + 
     geom_point(position = position_dodge(0.9), size = 1, alpha = 0.01) +
      geom_boxplot(alpha =0.1,position = position_dodge(0.9))+
      geom_hline(yintercept = 0, linetype = 'dotted') +
      scale_color_solarized() +
      facet_grid(clue ~ representation, scales = 'free') +
      labs(x = "", y = "cosine similarity") +
      theme_few()+
      theme(aspect.ratio = 1)
ggsave('by-difficulty.pdf')

c.sims %>%
  group_by(Subject, Trial, Level, CorrectAnswer, representation, comparison) %>%
  pivot_wider(names_from = comparison, values_from = sim) %>%
  group_by(representation, Trial,Level, CorrectAnswer, Subject) %>%
  transmute(c1w2 = max(c1w1_sim, c1w2_sim) - min(c1w1_sim, c1w2_sim),
            midpoint = c1avg_sim - min(c1w1_sim, c1w2_sim)) %>%
  pivot_longer(cols = -c(Subject, Trial, Level, CorrectAnswer, representation), names_to = 'comparison') %>%
  nest(-representation) %>% 
  mutate(results = map(data, ~ {
     lmer(value ~ comparison * Level + (1 | Subject) + (1 + comparison | CorrectAnswer), 
          data = .) %>% 
      broom.mixed::tidy()
    })) %>%
  unnest(results) %>%
  filter(effect == 'fixed')
```

Subject-level regression

```{r}
c.subj.df.c1 <- clue.dist %>%
  filter(clue == 'c1') %>% 
  filter(comparison != 'baseline') %>%
  pivot_wider(names_from = comparison, values_from = value) 

## model-wise
bert_c1 = c.subj.df.c1 %>% filter(representation %in% "BERT") 
swow_c1 = c.subj.df.c1 %>% filter(representation %in% "SWOW")
glove_c1 = c.subj.df.c1 %>% filter(representation %in% "GloVe")

t.test(bert_c1$word1, bert_c1$word2, paired = TRUE) #n.s.
t.test(bert_c1$word2, bert_c1$midpoint, paired = TRUE) #sig**

t.test(glove_c1$word1, glove_c1$word2, paired = TRUE) #n.s.
t.test(glove_c1$word2, glove_c1$midpoint, paired = TRUE) #sig**

t.test(swow_c1$word1, swow_c1$word2, paired = TRUE) #n.s.
t.test(swow_c1$word2, swow_c1$midpoint, paired = TRUE) #sig**
```

### Clue 2

```{r}

c.dist.df.c2 <- c.sims %>%
  group_by(Subject, representation, comparison) %>%
  summarise(sim = mean(sim, na.rm = TRUE)) %>%
 pivot_wider(names_from = comparison, values_from = sim) %>%
   mutate(baseline = (c2w1_sim + c2w2_sim) / 2) %>%
   pivot_longer(cols = -c(Subject, representation, baseline), names_to = 'comparison') %>%
   mutate(sim = value - baseline) %>%
   group_by(representation, comparison)

## restrict to clue2 metrics only

c.dist.df.c2 = c.dist.df.c2 %>%
  filter(comparison %in% c("c2w1_sim","c2w2_sim", "c2_w1w2avg_sim", "c2_c1avg_sim"))%>%
  select(-c(value, baseline)) %>%
  pivot_wider(names_from = comparison, values_from = sim) 

## model-wise
bert_c2 = c.dist.df.c2 %>% filter(representation %in% "bert-sum")
swow_c2 = c.dist.df.c2 %>% filter(representation %in% "swow")
glove_c2 = c.dist.df.c2 %>% filter(representation %in% "glove")

## bert
t.test(bert_c2$c2w1_sim, bert_c2$c2w2_sim, paired = TRUE) #sig**
t.test(bert_c2$c2w1_sim, bert_c2$c2_w1w2avg_sim, paired = TRUE) #sig**
t.test(bert_c2$c2w2_sim, bert_c2$c2_w1w2avg_sim, paired = TRUE) #sig**
t.test(bert_c2$c2_c1avg_sim, bert_c2$c2_w1w2avg_sim, paired = TRUE) #sig**

## glove
t.test(glove_c2$c2w1_sim, glove_c2$c2w2_sim, paired = TRUE) #n.s.
t.test(glove_c2$c2w1_sim, glove_c2$c2_w1w2avg_sim, paired = TRUE) #sig**
t.test(glove_c2$c2w2_sim, glove_c2$c2_w1w2avg_sim, paired = TRUE) #sig**
t.test(glove_c2$c2_c1avg_sim, glove_c2$c2_w1w2avg_sim, paired = TRUE) #sig**

## swow
t.test(swow_c2$c2w1_sim, swow_c2$c2w2_sim, paired = TRUE) #n.s.
t.test(swow_c2$c2w1_sim, swow_c2$c2_w1w2avg_sim, paired = TRUE) #sig**
t.test(swow_c2$c2w2_sim, swow_c2$c2_w1w2avg_sim, paired = TRUE) #sig**
t.test(swow_c2$c2_c1avg_sim, swow_c2$c2_w1w2avg_sim, paired = TRUE) #sig** but opposite direction
## clue2 closer to midpoint than centroid in SWOW

```

## Fig 3: Second Attempt (Clue2)

Here we restrict analyses to only those trials where at least one of the target words was guessed correctly.

```{r}

clue2_main = read_csv(here("../data/descriptive_precomputed.csv")) %>% filter(Clue2 != "na" & Clue2 != "") %>%
  rowwise() %>% 
  mutate(w1_guessed = as.numeric(str_detect(Word1, P2W1) | str_detect(Word1, P2W2)),
         w2_guessed = as.numeric(str_detect(Word2, P2W1) | str_detect(Word2, P2W2)),
         A1PartCorrect = ifelse( (w1_guessed == 1 & w2_guessed == 0), "w2 incorrect",
                                 ifelse((w1_guessed == 0 & w2_guessed == 1),
                                     "w1 incorrect", "both incorrect"))) %>%
  filter(A1PartCorrect != "both incorrect")
  
## glove
clue2_main$glove_c2_sim_guessed = ifelse(clue2_main$w1_guessed == 1, 
                                      clue2_main$glove_c2w1_sim, clue2_main$glove_c2w2_sim)
clue2_main$glove_c2_sim_unguessed = ifelse(clue2_main$w1_guessed == 0, 
                                      clue2_main$glove_c2w1_sim, clue2_main$glove_c2w2_sim)
clue2_main$glove_c2_sim_composite = clue2_main$glove_c2_w1w2avg_sim

## swow

clue2_main$swow_c2_sim_guessed = ifelse(clue2_main$w1_guessed == 1, 
                                      clue2_main$swow_c2w1_sim, clue2_main$swow_c2w2_sim)
clue2_main$swow_c2_sim_unguessed = ifelse(clue2_main$w1_guessed == 0, 
                                      clue2_main$swow_c2w1_sim, clue2_main$swow_c2w2_sim)
clue2_main$swow_c2_sim_composite = clue2_main$swow_c2_w1w2avg_sim

## bert

clue2_main$bert_c2_sim_guessed = ifelse(clue2_main$w1_guessed == 1, 
                                      clue2_main$`bert-sum_c2w1_sim`,
                                      clue2_main$`bert-sum_c2w2_sim`)
clue2_main$bert_c2_sim_unguessed = ifelse(clue2_main$w1_guessed == 0, 
                                      clue2_main$`bert-sum_c2w1_sim`,
                                      clue2_main$`bert-sum_c2w2_sim`)
clue2_main$bert_c2_sim_composite = clue2_main$`bert-sum_c2_w1w2avg_sim`

## selecting only similarities
clue2_sims = clue2_main %>% select(Subject, Word1, Word2, Clue1, Clue2,
                             glove_c2_sim_guessed:bert_c2_sim_composite) %>%
  pivot_longer(cols = -c(Subject, Word1, Word2, Clue1, Clue2), names_to = 'comparison')%>%
   separate(comparison, into = c("Representation", "Clue", "Sim", "Type"))

sims_rmisc= Rmisc::summarySE(clue2_sims,
                             measurevar = "value",
                             groupvars = c("Representation", "Type", "Clue"), na.rm = TRUE) 

sims_rmisc$Type = ordered(as.factor(sims_rmisc$Type), 
                          levels = c("guessed", "unguessed", "composite"))

sims_rmisc %>% filter(Clue == "c2") %>%
  mutate(Representation = fct_recode(Representation, 
                                     BERT = "bert", GloVe = "glove", SWOW = "swow"),
         Representation = fct_relevel(Representation, "GloVe", "BERT", "SWOW")) %>%
ggplot(aes(x = Type, y = value,
           group = Representation, color = Representation)) + 
  geom_point(size = 2) + 
  geom_line(size = 1)+
 geom_errorbar(aes(ymin=value - se, ymax=value + se), 
             size = 0.5, width=0, 
                  color = "gray26", position = position_dodge(0))+
 theme_few()+
  scale_color_solarized()+
    labs(x = "", y = "cosine similarity") +
    ggtitle("Clue 2")+
    theme_few()+
  ggtitle("Similarity of Clue 2")+
   theme( axis.text.x = element_text(face = "bold", size = rel(2)),
         axis.text.y = element_text(face = "bold", size = rel(2)),
         strip.text.y = element_text(face = "bold", size = rel(1.3)),
     axis.title = element_text(face = "bold", size = rel(2)),
          legend.title = element_text(face = "bold", size = rel(2)),
     legend.text = element_text(face = "bold", size = rel(1.8)),
         plot.title = element_text(hjust = .5),
         strip.text.x = element_text(face = "bold", size = rel(1.4)))

```

## Clue 2 Tests

```{r}
clue2_testdata =  clue2_sims %>%
  filter(Clue == "c2") %>%
  pivot_wider(names_from = Type, values_from = value  )

clue2_bert = clue2_testdata %>% filter(Representation == "bert")
clue2_glove = clue2_testdata %>% filter(Representation == "glove")
clue2_swow = clue2_testdata %>% filter(Representation == "swow")

## clue2 closer to unguessed vs. guessed vs. composite

t.test(clue2_bert$unguessed, clue2_bert$guessed) # sig.
t.test(clue2_bert$composite, clue2_bert$unguessed) #sig.


t.test(clue2_glove$unguessed, clue2_glove$guessed) # sig.
t.test(clue2_glove$composite, clue2_glove$unguessed) #sig.


t.test(clue2_swow$unguessed, clue2_swow$guessed) # sig
t.test(clue2_swow$composite, clue2_swow$unguessed) #sig., opposite direction
```

# Model comparison

Import the combined speaker predictions for all models (generated by `model_results.ipynb` notebook).

```{r}
board_predictions <- read_csv("speaker_boardfunc_df_ranks_softmax.csv") %>%
  right_join(c_clues %>% select(wordpair, Clue1, clueCount)) %>%
  filter(!is.na(Model)) %>%
  mutate(Model = ifelse(Model == 'SWOW-RW', 'swow', tolower(Model))) %>%
  unite(Model, Model, alpha, sep = '-alpha') %>%
  select(-X1, -boardnames)

rsa_predictions <- read_csv("speaker_ranks.csv") %>%
  mutate(Model = paste0(representation, '-alphaRSA'),
         prag_speaker_rank = gsub('\\[', '', prag_speaker_rank),
         prag_speaker_rank = as.numeric(gsub('\\]', '', prag_speaker_rank))) %>%
  rename(clue_score = prag_speaker_probs,
         clue_rank = prag_speaker_rank) %>%
  select(Model, wordpair, Clue1, clue_score, clueCount, clue_rank)

speaker_compiled <- bind_rows(board_predictions, rsa_predictions) %>%
  mutate(wordpair = gsub(' - ', '-', wordpair)) %>%
  filter(!is.na(clue_score))
```

Import the combined listener predictions for all models (generated by `model_results.ipynb` notebook).

```{r}
guesser_compiled <- read_csv('guesser_scores.csv') %>%
  mutate(true_wordpair = gsub( " ", "", wordpair)) %>%
  group_by(representation, Clue1, wordpair) %>%
  mutate(prag_rank = dense_rank(-prag_likelihood),
         literal_rank = dense_rank(-literal_likelihood),
         literal_top_rank = literal_rank == 1, 
         prag_top_rank = prag_rank == 1) %>%
  right_join(c, by = c('Clue1', 'true_wordpair')) %>%
  filter(!is.na(literal_top_prediction)) %>%
  rowwise() %>%
  filter(possible_wordpair == chosen_wordpair1 || possible_wordpair == chosen_wordpair2) %>%
  distinct()
```

## Speaker Predictions

### Table 3: Log likelihoods 

we look at log likelihoods as the most fundamental metric of model comparison

```{r}
speaker_compiled %>% 
  mutate(ll = log(clue_score)) %>%
  group_by(Model) %>%
  summarize(ll = sum(ll * clueCount, na.rm = T)) %>%
  arrange(-ll) %>%
  separate(Model, into = c('model', 'alpha'), sep = '-alpha') %>%
  mutate(category = case_when(alpha == 'RSA' ~ 'RSA',
                              alpha == '1' ~ 'target',
                              TRUE ~ 'board')) %>%
  group_by(model, category) %>%
  filter(max(ll) == ll)

```

### Average ranks (Figure 5)

for a more interpretable measure, we look at the average position of the empirical scores in the model's rankings. this gives some 'absolute' sense of how good these models are doing. Note that we're using a weighted average here. 

```{r}
speaker_compiled %>%
  group_by(Model, wordpair) %>%
  summarize(avg_rank = sum(clue_rank * clueCount)/sum(clueCount)) %>%
  separate(Model, into = c('model', 'alpha'), sep = '-alpha') %>%
  mutate(model = ifelse(model == 'bert-sum', 'BERT', ifelse(model == 'swow', 'SWOW', model))) %>%
  filter(alpha %in% c(0, 0.2, 0.4, 0.6, 0.8, 1, 'RSA')) %>%
  ggplot(aes(x = alpha, y = log(avg_rank), color = alpha == 'RSA')) +
    geom_jitter(width = 0.1, height = 0) +
    geom_boxplot(alpha = .5) +
    geom_hline(yintercept = c(0, log(12100)), linetype = 'dotted') +
    labs(y = '(log) rank of clues') +
    facet_grid(~ model) +
    ylim(0, 9.5) +
    theme_few() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
         aspect.ratio = 1.5,
         legend.position = 'none') 

ggsave('avg_ranking.pdf', width = 4, height = 2.5, units = 'in')
```

### Average ranks (Table 3)

```{r}
speaker_compiled %>%
  group_by(Model) %>%
  tidyboot_mean(sum(clue_rank * clueCount)/sum(clueCount)) %>%
  separate(Model, into = c('model', 'alpha'), sep = '-alpha') %>%
  mutate(category = case_when(alpha == 'RSA' ~ 'RSA',
                              alpha == '1' ~ 'target',
                              TRUE ~ 'board')) %>%
  group_by(model, category) %>%
  filter(min(empirical_stat) == empirical_stat)
```

### Top-5 accuracy  (Table 3)

for the table, we look up the proportion of items where the single empirically most frequent word matches the model's single top prediction (well, ok, technically we're looking at the top 5 for simplicity because we didn't exclude the target words themselves from the ranking lists...)

```{r}
# tidyboot_mean is bootstrapping 95% CIs
speaker_compiled %>%
  mutate(top = clue_rank < 5) %>%
  group_by(Model) %>%
  tidyboot_mean(sum(top * clueCount)/sum(clueCount)) %>%
  separate(Model, into = c('type', 'alpha'), sep = '-alpha') %>%
  mutate(category = case_when(alpha == 'RSA' ~ 'RSA',
                              alpha == '1' ~ 'target',
                              TRUE ~ 'board')) %>%
  group_by(type, category) %>%
  filter(max(empirical_stat) == empirical_stat)
```

look at some examples of modal clues (e.g. *glove* models predict 'blow' and 'boat' better but *swow* predicts 'math' and 'test' better)

```{r}
# These are the ones where one of our models gave a top rank
speaker_compiled %>% 
  group_by(wordpair) %>%
  top_n(1, clueCount) %>%
  select(wordpair, clue_rank, Model, Clue1) %>%
  filter(Model %in% c('glove-alpha1', 'swow-alpha1', 'glove-alphaRSA', 'swow-alphaRSA', 'bert-sum-alpha1', 'bert-sum-alphaRSA')) %>%
  filter(!str_detect(wordpair, Clue1)) %>%
  filter(clue_rank == 1)

speaker_compiled %>% 
  filter(wordpair %in% c("exam-algebra")) %>%
  arrange(wordpair, clue_rank)

speaker_compiled %>% 
  filter(wordpair %in% c("feet-chapel")) %>%
  arrange(wordpair, clue_rank)

```

## Guesser predictions

### Log-likelihood (Table 4)

```{r}
guesser_compiled %>% 
  mutate(prag_ll = log(prag_likelihood),
         literal_ll = log(literal_likelihood)) %>%
  gather(process_type, ll, prag_ll, literal_ll) %>%
  group_by(representation, process_type) %>%
  summarize(ll = sum(ll, na.rm = T)) %>%
  arrange(-ll)
```

### Ranks (Table 4)

```{r}
guesser_compiled %>%
  group_by(representation) %>%
  select(possible_wordpair, chosen_wordpair1, Clue1, ends_with('rank')) %>%
  gather(model, rank, literal_rank, prag_rank) %>%
  group_by(representation, model) %>%
  tidyboot_mean(rank)
```

### Accuracy (Table 4)

```{r}
guesser_compiled %>%
  # group_by(representation, Board, Clue1, chosen_wordpair1, literal_top_rank, prag_top_rank) %>%
  # tally() %>%
  # filter(n == max(n)) %>%
  group_by(representation) %>%
  select(chosen_wordpair1, Clue1, literal_top_rank, prag_top_rank) %>%
  gather(model, top, literal_top_rank, prag_top_rank) %>%
  group_by(representation, model) %>%
  tidyboot_mean(top)
```

# Supplemental

## Performance across time

are people getting better at this?

```{r}
## each dyad has 30 rows corresponding to the 30 rounds they played
## is performance getting better over time within a dyad?
c = read.csv("cogsci_descriptive.csv", header = TRUE, sep = ",")
c$wordpair = paste(c$Word1, "-", c$Word2)

# Attempts to guess the targets
library(tidyverse)
library(dplyr)
c$Attempts = ifelse(c$Player2.ACC == 1, 1, ifelse(c$Player2SecondAnswer.ACC == 1, 2, 3))

acc = c %>% select(Subject, Trial, wordpair,  Attempts) %>% arrange(Subject, Trial) %>% 
  group_by(Trial) %>%
  summarise_at(vars(Attempts), mean) 
acc = acc %>%
  mutate(Trials = as.numeric(rownames(acc)))

Hmisc::rcorr(acc$Trials, acc$Attempts) ## n.s. correlation

# Time taken by Speaker to generate clues
speaker_rt = c %>% select(Subject, Trial, wordpair, Player1.RT) %>% arrange(Subject, Trial) %>%
  group_by(Trial) %>%
  summarise_at(vars(Player1.RT), mean) 
speaker_rt = speaker_rt%>%
  mutate(Trials = as.numeric(rownames(speaker_rt)))
Hmisc::rcorr(speaker_rt$Trials, speaker_rt$Player1.RT) ## n.s. correlation

# Time taken by Guesser to make first guesses

guesser_rt = c %>% select(Subject, Trial, wordpair, Player2.RT) %>% arrange(Subject, Trial) %>%
  group_by(Trial) %>%
  summarise_at(vars(Player2.RT), mean)
guesser_rt = guesser_rt %>%
  mutate(Trials = as.numeric(rownames(guesser_rt)))
Hmisc::rcorr(guesser_rt$Trials, guesser_rt$Player2.RT) ## n.s. correlation

```

## Error analysis

it's interesting that RSA model seems to do slightly worse than the 'pure' alpha = 1 model that completely ignores distractors. let's look at some specific cases to try to understand the errors the RSA model might be making. 

first of all, we see that predictions are pretty highly correlated (r = 0.71) at the item-level.

```{r}
speaker_compiled %>%
  filter(Model %in% c('swow-alpha1', 'swow-alphaRSA')) %>%
  group_by(Model, wordpair) %>%
  summarize(avg_rank = sum(clue_rank * clueCount)/sum(clueCount)) %>%
  select(Model, avg_rank, wordpair) %>%
  spread(Model, avg_rank) %>%
  select(-wordpair) %>%
  ggpairs(mapping = aes(alpha = 0.01), lower = list(continuous = "smooth"), progress = F) +
    ggthemes::theme_few()
ggsave('correlations.pdf', width = 10, height = 10, units = 'in')
```

but RSA is *terrible* for some clues. for pairs like 'communicate - cooking', it seems like RSA doesn't want to produce clues like 'food' or 'kitchen' that are good for 'cooking' but not 'communicate'. it ranks these almost at the very bottom. this is more similar to what the alpha = 0.8 model is doing. 

```{r}
speaker_compiled %>%
  filter(Model %in% c('swow-alpha1', 'swow-alpha0.8', 'swow-alphaRSA')) %>%
  filter(wordpair == 'communicate - cooking') %>%
  select(Model, Clue1, clue_rank, wordpair) %>%
  spread(Model, clue_rank)
```


