---
title: "Connector-RSA"
output: pdf_document
---

# Imports

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidyboot)
library(ggthemes)
library(GGally)
```

clean up empirical data from the connector games.

```{r}
c <- read_csv("connector_maindata_sims.csv") %>%
  filter(!is.na(P2W1)) %>%
  unite(wordpair, Word1, Word2, sep = '-') %>%
  unite(chosen_wordpair1, P2W1, P2W2, sep = '-', remove = F) %>%
  unite(chosen_wordpair2, P2W2, P2W1, sep = '-', remove = T) %>%
  select(Subject, Experiment, wordpair, Trial, Clue1, chosen_wordpair1, chosen_wordpair2) %>%
  rename(true_wordpair = wordpair)

c_clues = read_csv("final_board_clues_all.csv") %>% 
  mutate(wordpair = gsub(' - ', '-', wordpair)) %>%
  distinct()
```

# Behavioral results

We import the csv generated by the `descriptive_analyses.ipynb` notebook.

```{r}
c.sims = read_csv("../data/descriptive_precomputed.csv") %>%
  unite(Word1, Word2, sep = '-') %>%
  pivot_longer(names_to = 'comparison', values_to = 'sim', cols = `bert-sum_w1w2_sim`:`swow_c2_c1avg_sim`) %>%
  separate(comparison, sep='_', into = c('representation', 'comparison'), extra = 'merge')
```

## Fig 1: Clue 1

is Clue1 close to W1, W2, or the Average

```{r}
c.dist <- c.sims %>%
  group_by(Subject, representation, comparison) %>%
  summarise(sim = mean(sim, na.rm = TRUE)) %>%
  pivot_wider(names_from = comparison, values_from = sim) %>%
  mutate(baseline = (c1w1_sim + c1w2_sim) / 2) %>%
  pivot_longer(cols = -c(Subject, representation, baseline), names_to = 'comparison') %>%
  mutate(sim = value - baseline) %>%
  group_by(representation, comparison) %>%
  tidyboot_mean(sim)

c.dist %>%
  filter(comparison %in% c("c1w1_sim","c1w2_sim", "c1avg_sim")) %>%
  mutate(comparison = fct_recode(comparison, word1 = "c1w1_sim", word2 = "c1w2_sim", midpoint = "c1avg_sim")) %>%
  ggplot(aes(x = comparison, y = empirical_stat, color = representation, group = representation)) + 
    geom_point(size = 2) + 
    geom_line(size = 1)+
    geom_hline(yintercept = 0, linetype = 'dotted') +
    geom_errorbar(aes(ymin=ci_lower, ymax=ci_upper), size = 1, width=0, 
                  color = "black", position = position_dodge(0))+
    labs(x = "comparisons", y = "cosine similarity") + 
    theme_few()
```

## Statistical tests

```{r}
## overall interaction
car::Anova(lme4::lmer(data = c1_dist, Similarity ~ Model*Pair + (1|Subject)))
## specific tests
c1_dist_test = group_by(c, Subject) %>%
  summarise_at(vars(glove_c1w1_sim, glove_c1w2_sim, glove_c1avg_sim,
                    swow_c1w1_sim, swow_c1w2_sim, swow_c1avg_sim,
                    bert_c1w1_sim, bert_c1w2_sim, bert_c1avg_sim), mean, na.rm = TRUE)
## clue1-word1 vs. clue1-word2: expected to be n.s.
t.test(c1_dist_test$glove_c1w1_sim, c1_dist_test$glove_c1w2_sim, paired = TRUE) #n.s.
t.test(c1_dist_test$swow_c1w1_sim, c1_dist_test$swow_c1w2_sim, paired = TRUE) #n.s.
t.test(c1_dist_test$bert_c1w1_sim, c1_dist_test$bert_c1w2_sim, paired = TRUE) #sig**

## clue1-word1 vs. clue1-avg: expected to be sig.
t.test(c1_dist_test$glove_c1w1_sim, c1_dist_test$glove_c1avg_sim, paired = TRUE) #sig.
t.test(c1_dist_test$swow_c1w1_sim, c1_dist_test$swow_c1avg_sim, paired = TRUE) #sig.
t.test(c1_dist_test$bert_c1w1_sim, c1_dist_test$bert_c1avg_sim, paired = TRUE) #sig.

## clue1-word2 vs. clue1-avg: expected to be sig.
t.test(c1_dist_test$glove_c1w2_sim, c1_dist_test$glove_c1avg_sim, paired = TRUE) #sig.
t.test(c1_dist_test$swow_c1w2_sim, c1_dist_test$swow_c1avg_sim, paired = TRUE) #sig.
t.test(c1_dist_test$bert_c1w2_sim, c1_dist_test$bert_c1avg_sim, paired = TRUE) #sig.

```

## Fig 2: Clue 2

```{r}
## is Clue2 close to W1/W2/W1+W2/W1+W2+C1

c2_dist = group_by(c, Subject) %>%
  summarise_at(vars(glove_c2w1_sim, glove_c2w2_sim, glove_c2_w1w2avg_sim, glove_c2_c1avg_sim,
                    swow_c2w1_sim, swow_c2w2_sim, swow_c2_w1w2avg_sim, swow_c2_c1avg_sim,
   bert_c2w1_sim, bert_c2w2_sim, bert_c2_w1w2avg_sim, bert_c2_c1avg_sim), mean, na.rm = TRUE) %>%
  gather(Model, Similarity, 
         glove_c2w1_sim, glove_c2w2_sim, glove_c2_w1w2avg_sim, glove_c2_c1avg_sim,
                    swow_c2w1_sim, swow_c2w2_sim, swow_c2_w1w2avg_sim, swow_c2_c1avg_sim,
   bert_c2w1_sim, bert_c2w2_sim, bert_c2_w1w2avg_sim, bert_c2_c1avg_sim) %>%
  separate(Model, into = c("Model", "Pair", "Avg", "Sim"))

c2_dist$Pair = ifelse(!c2_dist$Pair %in% c("c2w1", "c2w2"), c2_dist$Avg, c2_dist$Pair)
  
c2_rmisc =  Rmisc::summarySE(c2_dist,
                             measurevar = "Similarity", groupvars = c("Model", "Pair"))
c2_rmisc$Pair = ordered(as.factor(c2_rmisc$Pair), levels = c("c2w1", "c2w2", "w1w2avg", "c1avg"))
levels(c2_rmisc$Pair) = c("W1", "W2", "W1+W2", "W1+W2+C1")

c2_rmisc %>%
ggplot(aes(x = Pair, y = Similarity,
           group = Model, color = Model)) + 
  geom_point() + 
  geom_line()+
 geom_errorbar(aes(ymin=Similarity - se, ymax=Similarity + se),
             width=.01, color = "darkgray",
             position = position_dodge(0))+
 theme_few()+
       xlab("Vectors") + ylab("Cosine Similarity") +
  ggtitle("Clue2")+
   theme( axis.title = element_text(face = "bold", size = rel(1)),
          legend.title = element_text(face = "bold", size = rel(1)),
         plot.title = element_text(hjust = .5),
         strip.text.x = element_text(face = "bold", size = rel(1.4)))

```

# Pattern 3: Second Attempt

```{r}
clue2_data = c %>% filter(c$Clue2 != "na" & c$Clue2 != "")

clue2_data = clue2_data %>% 
  rowwise() %>% 
  mutate(w1_guessed = as.numeric(str_detect(Word1, P2W1) | str_detect(Word1, P2W2)),
         w2_guessed = as.numeric(str_detect(Word2, P2W1) | str_detect(Word2, P2W2)))

# create partcorrect variable
clue2_data$A1PartCorrect = ifelse( (clue2_data$w1_guessed == 1 & clue2_data$w2_guessed == 0), 
                                 "Attempt 2:\nWord2 Incorrect", 
                                 ifelse((clue2_data$w1_guessed == 0 & clue2_data$w2_guessed == 1),
                                     "Attempt 2:\nWord1 Incorrect", "Attempt 2:\nBoth Incorrect"))

## need to obtain similarities to guessed/unguessed/composite
## first we look at data where only 1 word was correct
w1_sims = clue2_data %>% filter(A1PartCorrect != "Attempt 2:\nBoth Incorrect")

## glove
# clue1 sims
w1_sims$glove_c1_sim_guessed = ifelse(w1_sims$w1_guessed == 1, 
                                      w1_sims$glove_c1w1_sim, w1_sims$glove_c1w2_sim)
w1_sims$glove_c1_sim_unguessed = ifelse(w1_sims$w1_guessed == 0, 
                                      w1_sims$glove_c1w1_sim, w1_sims$glove_c1w2_sim)
w1_sims$glove_c1_sim_composite = w1_sims$glove_c1avg_sim

# clue2sims
w1_sims$glove_c2_sim_guessed = ifelse(w1_sims$w1_guessed == 1, 
                                      w1_sims$glove_c2w1_sim, w1_sims$glove_c2w2_sim)
w1_sims$glove_c2_sim_unguessed = ifelse(w1_sims$w1_guessed == 0, 
                                      w1_sims$glove_c2w1_sim, w1_sims$glove_c2w2_sim)
w1_sims$glove_c2_sim_composite = w1_sims$glove_c2_w1w2avg_sim

## swow
# clue1 sims
w1_sims$swow_c1_sim_guessed = ifelse(w1_sims$w1_guessed == 1, 
                                      w1_sims$swow_c1w1_sim, w1_sims$swow_c1w2_sim)
w1_sims$swow_c1_sim_unguessed = ifelse(w1_sims$w1_guessed == 0, 
                                      w1_sims$swow_c1w1_sim, w1_sims$swow_c1w2_sim)
w1_sims$swow_c1_sim_composite = w1_sims$swow_c1avg_sim

# clue2sims
w1_sims$swow_c2_sim_guessed = ifelse(w1_sims$w1_guessed == 1, 
                                      w1_sims$swow_c2w1_sim, w1_sims$swow_c2w2_sim)
w1_sims$swow_c2_sim_unguessed = ifelse(w1_sims$w1_guessed == 0, 
                                      w1_sims$swow_c2w1_sim, w1_sims$swow_c2w2_sim)
w1_sims$swow_c2_sim_composite = w1_sims$swow_c2_w1w2avg_sim

## bert
# clue1 sims
w1_sims$bert_c1_sim_guessed = ifelse(w1_sims$w1_guessed == 1, 
                                      w1_sims$bert_c1w1_sim, w1_sims$bert_c1w2_sim)
w1_sims$bert_c1_sim_unguessed = ifelse(w1_sims$w1_guessed == 0, 
                                      w1_sims$bert_c1w1_sim, w1_sims$bert_c1w2_sim)
w1_sims$bert_c1_sim_composite = w1_sims$bert_c1avg_sim

# clue2sims
w1_sims$bert_c2_sim_guessed = ifelse(w1_sims$w1_guessed == 1, 
                                      w1_sims$bert_c2w1_sim, w1_sims$bert_c2w2_sim)
w1_sims$bert_c2_sim_unguessed = ifelse(w1_sims$w1_guessed == 0, 
                                      w1_sims$bert_c2w1_sim, w1_sims$bert_c2w2_sim)
w1_sims$bert_c2_sim_composite = w1_sims$bert_c2_w1w2avg_sim

## selecting only similarities
w1_sims = w1_sims %>% select(Subject, Word1, Word2, Clue1, Clue2,
                             glove_w1w2_sim, swow_w1w2_sim, bert_w1w2_sim,
                             glove_c1_sim_guessed, glove_c1_sim_unguessed, glove_c1_sim_composite,
                  glove_c2_sim_guessed, glove_c2_sim_unguessed, glove_c2_sim_composite,
                  bert_c1_sim_guessed, bert_c1_sim_unguessed, bert_c1_sim_composite,
                  bert_c2_sim_guessed, bert_c2_sim_unguessed, bert_c2_sim_composite,
                  swow_c1_sim_guessed, swow_c1_sim_unguessed, swow_c1_sim_composite,
                  swow_c2_sim_guessed, swow_c2_sim_unguessed, swow_c2_sim_composite) %>% 
  gather(Model, Similarity, 
                  glove_c1_sim_guessed, glove_c1_sim_unguessed, glove_c1_sim_composite,
                  glove_c2_sim_guessed, glove_c2_sim_unguessed, glove_c2_sim_composite,
                  bert_c1_sim_guessed, bert_c1_sim_unguessed, bert_c1_sim_composite,
                  bert_c2_sim_guessed, bert_c2_sim_unguessed, bert_c2_sim_composite,
                  swow_c1_sim_guessed, swow_c1_sim_unguessed, swow_c1_sim_composite,
                  swow_c2_sim_guessed, swow_c2_sim_unguessed, swow_c2_sim_composite) %>%
  separate(Model, into = c("Model", "Clue", "Sim", "Type"))

## divide this up by how close w1 and w2 are across all models (bert/glove/swow)
w1_sims$overall_sim = (w1_sims$glove_w1w2_sim + w1_sims$swow_w1w2_sim+ w1_sims$bert_w1w2_sim)/3
w1_sims$Sim = ifelse(w1_sims$overall_sim <= median(w1_sims$overall_sim, na.rm = TRUE),
                     "Distant Targets", "Close Targets")

sims_rmisc= Rmisc::summarySE(w1_sims,
                             measurevar = "Similarity",
                             groupvars = c("Type", "Clue", "Sim"), na.rm = TRUE) 

sims_rmisc$Type = ordered(as.factor(sims_rmisc$Type), 
                          levels = c("guessed", "unguessed", "composite"))

sims_rmisc %>%
ggplot(aes(x = Type, y = Similarity,
           group = Clue, color = Clue)) + 
  geom_point(size = 4) + 
  geom_line()+
 geom_errorbar(aes(ymin=Similarity - se, ymax=Similarity + se), 
             width=.01, color = "black",
             position = position_dodge(0))+
 theme_few()+
  facet_wrap(~Sim)+
       xlab("Vectors") + ylab("Cosine Similarity") +
  ggtitle("")+
   theme( axis.title = element_text(face = "bold", size = rel(1)),
          legend.title = element_text(face = "bold", size = rel(1)),
         plot.title = element_text(hjust = .5),
         strip.text.x = element_text(face = "bold", size = rel(1.4)))
```

## Clue 2 Tests

```{r}
clue2_testdata =  clue2_data %>% filter(A1PartCorrect != "Attempt 2:\nBoth Incorrect")

## glove
# clue2sims
clue2_testdata$glove_c2_sim_guessed = ifelse(clue2_testdata$w1_guessed == 1, 
                                      clue2_testdata$glove_c2w1_sim, clue2_testdata$glove_c2w2_sim)
clue2_testdata$glove_c2_sim_unguessed = ifelse(clue2_testdata$w1_guessed == 0, 
                                      clue2_testdata$glove_c2w1_sim, clue2_testdata$glove_c2w2_sim)
clue2_testdata$glove_c2_sim_compositew1w2 = clue2_testdata$glove_c2_w1w2avg_sim
clue2_testdata$glove_c2_sim_compositew1w2c1 = clue2_testdata$glove_c2_c1avg_sim

## swow

clue2_testdata$swow_c2_sim_guessed = ifelse(clue2_testdata$w1_guessed == 1, 
                                      clue2_testdata$swow_c2w1_sim, clue2_testdata$swow_c2w2_sim)
clue2_testdata$swow_c2_sim_unguessed = ifelse(clue2_testdata$w1_guessed == 0, 
                                      clue2_testdata$swow_c2w1_sim, clue2_testdata$swow_c2w2_sim)
clue2_testdata$swow_c2_sim_compositew1w2 = clue2_testdata$swow_c2_w1w2avg_sim
clue2_testdata$swow_c2_sim_compositew1w2c1 = clue2_testdata$swow_c2_c1avg_sim

## bert

clue2_testdata$bert_c2_sim_guessed = ifelse(clue2_testdata$w1_guessed == 1, 
                                      clue2_testdata$bert_c2w1_sim, clue2_testdata$bert_c2w2_sim)
clue2_testdata$bert_c2_sim_unguessed = ifelse(clue2_testdata$w1_guessed == 0, 
                                      clue2_testdata$bert_c2w1_sim, clue2_testdata$bert_c2w2_sim)
clue2_testdata$bert_c2_sim_compositew1w2 = clue2_testdata$bert_c2_w1w2avg_sim
clue2_testdata$bert_c2_sim_compositew1w2c1 = clue2_testdata$bert_c2_c1avg_sim



## selecting only similarities
clue2_testdata = clue2_testdata %>% select(Subject, Word1, Word2, Clue1, Clue2,
                             glove_w1w2_sim, swow_w1w2_sim, bert_w1w2_sim,
                  glove_c2_sim_guessed, glove_c2_sim_unguessed, 
                  glove_c2_sim_compositew1w2,glove_c2_sim_compositew1w2c1,
                  
                  bert_c2_sim_guessed, bert_c2_sim_unguessed, 
                  bert_c2_sim_compositew1w2,bert_c2_sim_compositew1w2c1, 
                  
                  swow_c2_sim_guessed, swow_c2_sim_unguessed, 
                  swow_c2_sim_compositew1w2, swow_c2_sim_compositew1w2c1) 
## clue2 closer to unguessed vs. guessed?

t.test(clue2_testdata$glove_c2_sim_unguessed, clue2_testdata$glove_c2_sim_guessed) #sig.
t.test(clue2_testdata$swow_c2_sim_unguessed, clue2_testdata$swow_c2_sim_guessed) #sig.
t.test(clue2_testdata$bert_c2_sim_unguessed, clue2_testdata$bert_c2_sim_guessed) #sig.

## closer to composite W1W2 vs. unguessed?
t.test(clue2_testdata$glove_c2_sim_compositew1w2, clue2_testdata$glove_c2_sim_unguessed) 
#sig.
t.test(clue2_testdata$swow_c2_sim_compositew1w2, clue2_testdata$swow_c2_sim_unguessed) 
#sig. # opposite direction, i.e., closer to unguessed than composite
t.test(clue2_testdata$bert_c2_sim_compositew1w2, clue2_testdata$bert_c2_sim_unguessed) 
#sig.

## closer to composite W1WwC1 vs. composite W1W2?

t.test(clue2_testdata$glove_c2_sim_compositew1w2c1, clue2_testdata$glove_c2_sim_compositew1w2) 
#sig.
t.test(clue2_testdata$swow_c2_sim_compositew1w2c1, clue2_testdata$swow_c2_sim_compositew1w2) 
#sig. # opposite direction, i.e., closer to w1w2 composite than w1w2c1
t.test(clue2_testdata$bert_c2_sim_compositew1w2c1, clue2_testdata$bert_c2_sim_compositew1w2) 
#sig.

```

# Model comparison

Import the combined speaker predictions for all models (generated by `model_results.ipynb` notebook).

```{r}
board_predictions <- read_csv("speaker_boardfunc_df_ranks_softmax.csv") %>%
  right_join(c_clues %>% select(wordpair, Clue1, clueCount)) %>%
  filter(!is.na(Model)) %>%
  mutate(Model = ifelse(Model == 'SWOW-RW', 'swow', tolower(Model))) %>%
  unite(Model, Model, alpha, sep = '-alpha') %>%
  select(-X1, -boardnames)

rsa_predictions <- read_csv("speaker_ranks.csv") %>%
  mutate(Model = paste0(representation, '-alphaRSA'),
         prag_speaker_rank = gsub('\\[', '', prag_speaker_rank),
         prag_speaker_rank = as.numeric(gsub('\\]', '', prag_speaker_rank))) %>%
  rename(clue_score = prag_speaker_probs,
         clue_rank = prag_speaker_rank) %>%
  select(Model, wordpair, Clue1, clue_score, clueCount, clue_rank)

speaker_compiled <- bind_rows(board_predictions, rsa_predictions) %>%
  mutate(wordpair = gsub(' - ', '-', wordpair)) %>%
  filter(!is.na(clue_score))
```

Import the combined listener predictions for all models (generated by `model_results.ipynb` notebook).

```{r}
guesser_compiled <- read_csv('guesser_scores.csv') %>%
  mutate(true_wordpair = gsub( " ", "", wordpair)) %>%
  group_by(representation, Clue1, wordpair) %>%
  mutate(prag_rank = dense_rank(-prag_likelihood),
         literal_rank = dense_rank(-literal_likelihood),
         literal_top_rank = literal_rank == 1, 
         prag_top_rank = prag_rank == 1) %>%
  right_join(c, by = c('Clue1', 'true_wordpair')) %>%
  filter(!is.na(literal_top_prediction)) %>%
  rowwise() %>%
  filter(possible_wordpair == chosen_wordpair1 || possible_wordpair == chosen_wordpair2) %>%
  distinct()
```

## Speaker Predictions

### Table 3: Log likelihoods 

we look at log likelihoods as the most fundamental metric of model comparison

```{r}
speaker_compiled %>% 
  mutate(ll = log(clue_score)) %>%
  group_by(Model) %>%
  summarize(ll = sum(ll * clueCount, na.rm = T)) %>%
  arrange(-ll) %>%
  separate(Model, into = c('model', 'alpha'), sep = '-alpha') %>%
  mutate(category = case_when(alpha == 'RSA' ~ 'RSA',
                              alpha == '1' ~ 'target',
                              TRUE ~ 'board')) %>%
  group_by(model, category) %>%
  filter(max(ll) == ll)

```

### Average ranks (Figure 5)

for a more interpretable measure, we look at the average position of the empirical scores in the model's rankings. this gives some 'absolute' sense of how good these models are doing. Note that we're using a weighted average here. 

```{r}
speaker_compiled %>%
  group_by(Model, wordpair) %>%
  summarize(avg_rank = sum(clue_rank * clueCount)/sum(clueCount)) %>%
  separate(Model, into = c('model', 'alpha'), sep = '-alpha') %>%
  mutate(model = ifelse(model == 'bert-sum', 'BERT', ifelse(model == 'swow', 'SWOW', model))) %>%
  filter(alpha %in% c(0, 0.2, 0.4, 0.6, 0.8, 1, 'RSA')) %>%
  ggplot(aes(x = alpha, y = log(avg_rank), color = alpha == 'RSA')) +
    geom_jitter(width = 0.1, height = 0) +
    geom_boxplot(alpha = .5) +
    geom_hline(yintercept = c(0, log(12100)), linetype = 'dotted') +
    labs(y = '(log) rank of clues') +
    facet_grid(~ model) +
    ylim(0, 9.5) +
    theme_few() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
         aspect.ratio = 1.5,
         legend.position = 'none') 

ggsave('avg_ranking.pdf', width = 4, height = 2.5, units = 'in')
```

### Average ranks (Table 3)

```{r}
speaker_compiled %>%
  group_by(Model) %>%
  tidyboot_mean(sum(clue_rank * clueCount)/sum(clueCount)) %>%
  separate(Model, into = c('model', 'alpha'), sep = '-alpha') %>%
  mutate(category = case_when(alpha == 'RSA' ~ 'RSA',
                              alpha == '1' ~ 'target',
                              TRUE ~ 'board')) %>%
  group_by(model, category) %>%
  filter(min(empirical_stat) == empirical_stat)
```

### Top-5 accuracy  (Table 3)

for the table, we look up the proportion of items where the single empirically most frequent word matches the model's single top prediction (well, ok, technically we're looking at the top 5 for simplicity because we didn't exclude the target words themselves from the ranking lists...)

```{r}
# tidyboot_mean is bootstrapping 95% CIs
speaker_compiled %>%
  mutate(top = clue_rank < 5) %>%
  group_by(Model) %>%
  tidyboot_mean(sum(top * clueCount)/sum(clueCount)) %>%
  separate(Model, into = c('type', 'alpha'), sep = '-alpha') %>%
  mutate(category = case_when(alpha == 'RSA' ~ 'RSA',
                              alpha == '1' ~ 'target',
                              TRUE ~ 'board')) %>%
  group_by(type, category) %>%
  filter(max(empirical_stat) == empirical_stat)
```

look at some examples of modal clues (e.g. *glove* models predict 'blow' and 'boat' better but *swow* predicts 'math' and 'test' better)

```{r}
# These are the ones where one of our models gave a top rank
speaker_compiled %>% 
  group_by(wordpair) %>%
  top_n(1, clueCount) %>%
  select(wordpair, clue_rank, Model, Clue1) %>%
  filter(Model %in% c('glove-alpha1', 'swow-alpha1', 'glove-alphaRSA', 'swow-alphaRSA', 'bert-sum-alpha1', 'bert-sum-alphaRSA')) %>%
  filter(!str_detect(wordpair, Clue1)) %>%
  filter(clue_rank == 1)

speaker_compiled %>% 
  filter(wordpair %in% c("exam-algebra")) %>%
  arrange(wordpair, clue_rank)

speaker_compiled %>% 
  filter(wordpair %in% c("feet-chapel")) %>%
  arrange(wordpair, clue_rank)

```

## Guesser predictions

### Log-likelihood (Table 4)

```{r}
guesser_compiled %>% 
  mutate(prag_ll = log(prag_likelihood),
         literal_ll = log(literal_likelihood)) %>%
  gather(process_type, ll, prag_ll, literal_ll) %>%
  group_by(representation, process_type) %>%
  summarize(ll = sum(ll, na.rm = T)) %>%
  arrange(-ll)
```

### Ranks (Table 4)

```{r}
guesser_compiled %>%
  group_by(representation) %>%
  select(possible_wordpair, chosen_wordpair1, Clue1, ends_with('rank')) %>%
  gather(model, rank, literal_rank, prag_rank) %>%
  group_by(representation, model) %>%
  tidyboot_mean(rank)
```

### Accuracy (Table 4)

```{r}
guesser_compiled %>%
  # group_by(representation, Board, Clue1, chosen_wordpair1, literal_top_rank, prag_top_rank) %>%
  # tally() %>%
  # filter(n == max(n)) %>%
  group_by(representation) %>%
  select(chosen_wordpair1, Clue1, literal_top_rank, prag_top_rank) %>%
  gather(model, top, literal_top_rank, prag_top_rank) %>%
  group_by(representation, model) %>%
  tidyboot_mean(top)
```

# Supplemental

## Performance across time

are people getting better at this?

```{r}
## each dyad has 30 rows corresponding to the 30 rounds they played
## is performance getting better over time within a dyad?
c = read.csv("cogsci_descriptive.csv", header = TRUE, sep = ",")
c$wordpair = paste(c$Word1, "-", c$Word2)

# Attempts to guess the targets
library(tidyverse)
library(dplyr)
c$Attempts = ifelse(c$Player2.ACC == 1, 1, ifelse(c$Player2SecondAnswer.ACC == 1, 2, 3))

acc = c %>% select(Subject, Trial, wordpair,  Attempts) %>% arrange(Subject, Trial) %>% 
  group_by(Trial) %>%
  summarise_at(vars(Attempts), mean) 
acc = acc %>%
  mutate(Trials = as.numeric(rownames(acc)))

Hmisc::rcorr(acc$Trials, acc$Attempts) ## n.s. correlation

# Time taken by Speaker to generate clues
speaker_rt = c %>% select(Subject, Trial, wordpair, Player1.RT) %>% arrange(Subject, Trial) %>%
  group_by(Trial) %>%
  summarise_at(vars(Player1.RT), mean) 
speaker_rt = speaker_rt%>%
  mutate(Trials = as.numeric(rownames(speaker_rt)))
Hmisc::rcorr(speaker_rt$Trials, speaker_rt$Player1.RT) ## n.s. correlation

# Time taken by Guesser to make first guesses

guesser_rt = c %>% select(Subject, Trial, wordpair, Player2.RT) %>% arrange(Subject, Trial) %>%
  group_by(Trial) %>%
  summarise_at(vars(Player2.RT), mean)
guesser_rt = guesser_rt %>%
  mutate(Trials = as.numeric(rownames(guesser_rt)))
Hmisc::rcorr(guesser_rt$Trials, guesser_rt$Player2.RT) ## n.s. correlation

```

## Error analysis

it's interesting that RSA model seems to do slightly worse than the 'pure' alpha = 1 model that completely ignores distractors. let's look at some specific cases to try to understand the errors the RSA model might be making. 

first of all, we see that predictions are pretty highly correlated (r = 0.71) at the item-level.

```{r}
speaker_compiled %>%
  filter(Model %in% c('swow-alpha1', 'swow-alphaRSA')) %>%
  group_by(Model, wordpair) %>%
  summarize(avg_rank = sum(clue_rank * clueCount)/sum(clueCount)) %>%
  select(Model, avg_rank, wordpair) %>%
  spread(Model, avg_rank) %>%
  select(-wordpair) %>%
  ggpairs(mapping = aes(alpha = 0.01), lower = list(continuous = "smooth"), progress = F) +
    ggthemes::theme_few()
ggsave('correlations.pdf', width = 10, height = 10, units = 'in')
```

but RSA is *terrible* for some clues. for pairs like 'communicate - cooking', it seems like RSA doesn't want to produce clues like 'food' or 'kitchen' that are good for 'cooking' but not 'communicate'. it ranks these almost at the very bottom. this is more similar to what the alpha = 0.8 model is doing. 

```{r}
speaker_compiled %>%
  filter(Model %in% c('swow-alpha1', 'swow-alpha0.8', 'swow-alphaRSA')) %>%
  filter(wordpair == 'communicate - cooking') %>%
  select(Model, Clue1, clue_rank, wordpair) %>%
  spread(Model, clue_rank)
```


