{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "search-models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOl84/DPgIiLphbfaoRfgES",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhilasha-kumar/Connector/blob/master/search-models/search_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH3K3UX0r_uM"
      },
      "source": [
        "# Importing packages and mounting drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLHTgCLD-5VS"
      },
      "source": [
        "import networkx as nx\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import operator\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import heapq\n",
        "import pandas as pd\n",
        "import scipy.spatial.distance\n",
        "import numpy as np\n",
        "from numpy.random import randint\n",
        "from sklearn.preprocessing import MinMaxScaler, normalize\n",
        "from numpy.linalg import matrix_power\n",
        "from scipy import stats\n",
        "from sklearn import preprocessing\n",
        "from numpy.random import choice\n",
        "import json\n",
        "import heapq\n",
        "import json\n",
        "import itertools\n",
        "import sys\n",
        "import scipy.spatial.distance\n",
        "from scipy.special import softmax\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_BINlf1Pwzc",
        "outputId": "f1cbe73d-1cb2-4dfc-c59c-4a9a09df76c4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54ZMyOWfsEGS"
      },
      "source": [
        "# Importing embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxoEmRAHPuDn",
        "outputId": "bc83986c-3594-4ff0-e4c8-c25a63561492"
      },
      "source": [
        "# import glove embeddings\n",
        "representations = {}\n",
        "parentfolder = \"/content/drive/My Drive/Connector-CogSci2021/Descriptive analyses and model predictions/\"\n",
        "representations['glove'] = pd.read_csv(parentfolder +\"/glove_embeddings.csv\").transpose().values\n",
        "vocab = pd.DataFrame(list(pd.read_csv(parentfolder +\"/glove_embeddings.csv\").columns), columns=[\"vocab_word\"])\n",
        "print(f\"embeddings are shaped:\", representations['glove'].shape)\n",
        "print(f\"vocab is {len(vocab)} words\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embeddings are shaped: (12218, 300)\n",
            "vocab is 12218 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urqF5SZjsF7h"
      },
      "source": [
        "# Defining search functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxhSJwQlQcFO"
      },
      "source": [
        "def create_similarity_matrix(matrix, vector, N):\n",
        "  matrix = 1-scipy.spatial.distance.cdist(matrix, vector, 'cosine').reshape(-1)\n",
        "  matrix = matrix.reshape((N,N))\n",
        "  return matrix\n",
        "\n",
        "def create_graph(similarity_matrix, threshold):\n",
        "  ## make diagonal 0 and all values below a certain threshold also 0: can parameterize this eventually\n",
        "  x = np.copy(similarity_matrix)\n",
        "  np.fill_diagonal(x, 0)\n",
        "  x[x < threshold] = 0\n",
        "  G = nx.from_numpy_matrix(np.matrix(x), create_using=nx.DiGraph)\n",
        "  return G\n",
        "\n",
        "def random_walk(word, n_steps):\n",
        "  nodes_visited = []\n",
        "  random_node = list(vocab.vocab_word).index(word) \n",
        "  nodes_visited.append(random_node)\n",
        "  dict_counter = {} #initialise the value for all nodes as 0 (i.e., each node has been visited 0 times)\n",
        "  for i in range(Graph.number_of_nodes()):\n",
        "      dict_counter[i] = 0\n",
        "  # update dict_count for the chosen random node by 1 (since walk starts here)\n",
        "  dict_counter[random_node] = dict_counter[random_node]+1\n",
        "\n",
        "  #Traversing through the neighbors of start node\n",
        "  #increment by traversing through all neighbors nodes ()\n",
        "  for i in range(n_steps):\n",
        "      list_for_nodes = list(Graph.neighbors(random_node))\n",
        "      neighbors = {}\n",
        "      for n in list_for_nodes:\n",
        "        neighbors[n] = Graph.edges[(random_node,n)]['weight']\n",
        "      if len(list_for_nodes)==0:\n",
        "        # if random_node having no outgoing edges\n",
        "        # choose a different random node and update its visit count\n",
        "        random_node = random.choice([i for i in range(Graph.number_of_nodes())])\n",
        "        dict_counter[random_node] = dict_counter[random_node]+1\n",
        "        nodes_visited.append(random_node)\n",
        "      else:\n",
        "        #choose a node randomly from neighbors of current random_node\n",
        "        random_node = random.choices(population = list_for_nodes, k = 1,\n",
        "                weights=list(neighbors.values()))[0]\n",
        "        dict_counter[random_node] = dict_counter[random_node]+1\n",
        "        nodes_visited.append(random_node)\n",
        "          \n",
        "  ## return the words visited and the counts of each time a node was visited (in the order of vocab) as a numpy array\n",
        "\n",
        "  words_visited = [list(vocab.vocab_word)[index] for index in nodes_visited]\n",
        "\n",
        "  return words_visited, np.fromiter(dict_counter.values(), dtype=float)\n",
        "\n",
        "def union_intersection(w1,w2, n_steps, n_walks, vocabulary):\n",
        "  \"\"\" starts n_walks independent random walks for n_steps, and computes union and intersection \"\"\"\n",
        "  rw_w1 = np.sum(np.array([random_walk(w1, n_steps)[1] for i in range(n_walks)]), axis = 0)\n",
        "  rw_w2 = np.sum(np.array([random_walk(w2, n_steps)[1] for i in range(n_walks)]), axis = 0)\n",
        "\n",
        "  v = vocabulary.copy()\n",
        "\n",
        "  v[\"w1_visited_count\"] = rw_w1.tolist()\n",
        "  v[\"w2_visited_count\"] = rw_w2.tolist()\n",
        "\n",
        "  v[\"w1*w2\"] = v[\"w1_visited_count\"]*v[\"w2_visited_count\"]\n",
        "\n",
        "  ## compute non-zero, i.e., all visited nodes \n",
        "\n",
        "  nonzero_w1 = list(v.loc[v['w1_visited_count'] != 0].vocab_word)\n",
        "  nonzero_w2 = list(v.loc[v['w2_visited_count'] != 0].vocab_word)\n",
        "\n",
        "  ## compute union and intersection\n",
        "\n",
        "  union = set(nonzero_w1).union(set(nonzero_w2))\n",
        "  intersection = set(nonzero_w1).intersection(set(nonzero_w2))\n",
        "\n",
        "  ## we also need the counts of these visited nodes, sorted by nodes visited highly by both words\n",
        "\n",
        "  union_df = v.loc[v['vocab_word'].isin(list(union))].sort_values(by='w1*w2', ascending=False)\n",
        "  intersection_df = v.loc[v['vocab_word'].isin(list(intersection))].sort_values(by='w1*w2', ascending=False)\n",
        "\n",
        "  return union_df, intersection_df\n",
        "\n",
        "def predication_vector(sim_matrix, w1, w2, m, k):\n",
        "  \"\"\"\n",
        "  # computes a predication vector based on kintsch's predication algorithm\n",
        "  # in this algorithm, first m neighbors of w1 are computed\n",
        "  # a network is created using m, w1, and w2 with inhibitory links between the m nodes and all cosines b/w m and w1 and w2\n",
        "  # this network is then \"integrated\" until steady state, and then\n",
        "  # a centroid is calculated using w1, w2, and k strongest neighbors of w2\n",
        "  ## details of spreading activation:\n",
        "  # More specific, an activation vector representing the initial activation\n",
        "  # values of all nodes in the net is postmultiplied repeatedly with\n",
        "  # the connectivity matrix. After each multiplication the activation\n",
        "  # values are renormalized: Negative values are set to zero,\n",
        "  # and each of the positive activation values is divided by the sum\n",
        "  # of all activation values, so that the total activation on each cycles\n",
        "  # remains at a value of one (e.g., Rumelhart & McClelland,\n",
        "  # 1986). Usually, the system finds a stable state fairly rapidly;\n",
        "  \"\"\"\n",
        "  neighbors_of_w1 = sim_matrix[list(vocab.vocab_word).index(w1)]\n",
        "  topn_indices = np.argpartition(neighbors_of_w1, -m)[-m:].tolist()\n",
        "  nodes_indices = list(set(topn_indices + [list(vocab.vocab_word).index(w1),list(vocab.vocab_word).index(w2) ]))\n",
        "  nodes = [list(vocab.vocab_word)[i] for i in nodes_indices]\n",
        "  vecs = representations['glove'][nodes_indices]\n",
        "  # construct nodes similarity matrix \n",
        "  s = create_similarity_matrix(vecs, vecs, vecs.shape[0])\n",
        "  w1_index = nodes.index(w1)\n",
        "  w2_index = nodes.index(w2)\n",
        "  valid_indices = np.arange(0,s.shape[1]).tolist()\n",
        "  valid_indices.pop(w1_index)\n",
        "  valid_indices.pop(w2_index)\n",
        "  s[:, valid_indices] = 0\n",
        "  # also set connections to self = 0\n",
        "  s[w1_index, w1_index] = 0\n",
        "  s[w2_index, w2_index] = 0\n",
        "  n_zero = s.size - np.count_nonzero(s)\n",
        "  sum_positive_activations = np.sum(s)\n",
        "  # set all 0 values to sum/n_zero\n",
        "  s[s == 0] = -sum_positive_activations/n_zero\n",
        "\n",
        "  ## now we perform the spreading activation integration: restricted to 5 steps\n",
        "  s_integrated = kintsch_integration(s, 5)\n",
        "  ## now we extract k neighbors of w2\n",
        "\n",
        "  neighbors_of_w2 = s_integrated[w2_index]\n",
        "  topn_indices = np.argpartition(neighbors_of_w2, -k)[-k:].tolist()\n",
        "  w2_words = [nodes[i] for i in topn_indices]\n",
        "  indices_in_vocab = [list(vocab.vocab_word).index(i) for i in w2_words]\n",
        "  nodes_indices = list(set(indices_in_vocab + [list(vocab.vocab_word).index(w1),list(vocab.vocab_word).index(w2) ]))\n",
        "  finalnodes = [list(vocab.vocab_word)[i] for i in nodes_indices]\n",
        "  vecs = representations['glove'][nodes_indices]\n",
        "  ## next we compute centroid of w1, w2, and w2_words\n",
        "  centroid = np.mean(vecs, axis = 0).reshape((1,vecs.shape[1]))\n",
        "  close = find_closest(centroid, vocab, representations['glove'])\n",
        "  return  close\n",
        "  \n",
        "def kintsch_integration(m, n_times):\n",
        "  for i in range(1,n_times):\n",
        "    m = m**(i+1)\n",
        "    m[m <0] = 0\n",
        "    sum_positive_activations = np.sum(m)\n",
        "    m = m/sum_positive_activations\n",
        "  return m\n",
        "\n",
        "def find_closest(vector, vocab, embeddings, k = 10):\n",
        "  \"\"\"finds the words closest to given vector in a given vocab for given embeddings\"\"\"\n",
        "  cosine = 1-scipy.spatial.distance.cdist(embeddings, vector, 'cosine')\n",
        "  cosine = cosine.flatten()\n",
        "  centroid_indices = np.argpartition(cosine, -k)[-k:].tolist()\n",
        "  centroid_words = [list(vocab.vocab_word)[i] for i in centroid_indices]\n",
        "  return centroid_words    "
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predication_vector(sim_matrix_glove, \"happy\", \"sad\", 200, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9C0Duz60zzK",
        "outputId": "24fc201b-37a2-46ab-e68b-14ae88bb3e32"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['feel',\n",
              " 'always',\n",
              " 'love',\n",
              " 'really',\n",
              " 'sad',\n",
              " 'thing',\n",
              " 'happy',\n",
              " 'everyone',\n",
              " 'something',\n",
              " 'want']"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtuHKP7ItXDy"
      },
      "source": [
        "# Speaker functions\n",
        "These are directly copied over from the cogsci notebook except for the last two functions that restrict the RSA functions to set of candidates passed as function arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlh_haE7tazm"
      },
      "source": [
        "parentfolder = \"/content/drive/My Drive/Connector-CogSci2021/Descriptive analyses and model predictions/\"\n",
        "with open(parentfolder + 'boards.json', 'r') as json_file:\n",
        "    boards = json.load(json_file)\n",
        "\n",
        "expdata = pd.read_csv(parentfolder + \"final_board_clues_all.csv\", encoding= 'unicode_escape')\n",
        "sample_df = pd.read_csv(parentfolder + \"vocab.csv\")\n",
        "\n",
        "def compute_board_combos(board_name):\n",
        "    board = boards[board_name]\n",
        "    all_possible_combs = list(itertools.combinations(board, 2))\n",
        "    combs_df = pd.DataFrame(all_possible_combs, columns =['Word1', 'Word2'])\n",
        "    combs_df[\"wordpair\"] = combs_df[\"Word1\"] + '-'+ combs_df[\"Word2\"]\n",
        "    return combs_df\n",
        "\n",
        "allcombinations_df = pd.DataFrame(columns=['Board', 'Word1','Word2', 'wordpair'])\n",
        "board_combos = {board_name : compute_board_combos(board_name) for board_name in boards.keys()}\n",
        "\n",
        "for board in board_combos:\n",
        "    newdf = board_combos[board]\n",
        "    newdf.insert(loc=0, column='Board', value=board)\n",
        "    allcombinations_df = pd.concat([allcombinations_df, newdf])\n",
        "\n",
        "def get_wordpair_list(board_combos, board_name) :\n",
        "    return list(board_combos[board_name]['wordpair'])\n",
        "\n",
        "def create_board_matrix(combs_df, context_board, embeddings):\n",
        "    # grab subset of words in given board and their corresponding glove vectors\n",
        "    board_df = sample_df[sample_df['Word'].isin(context_board)]\n",
        "    board_word_indices = list(board_df.index)\n",
        "    board_words = board_df[\"Word\"]\n",
        "    board_vectors = embeddings[board_word_indices]\n",
        "\n",
        "    ## clue_sims is the similarity of ALL clues in full searchspace (size N) to EACH word on board (size 20)\n",
        "    clue_sims = 1 - scipy.spatial.distance.cdist(board_vectors, embeddings, 'cosine')\n",
        "\n",
        "    ## once we have the similarities of the clue to the words on the board\n",
        "    ## we define a multiplicative function that maximizes these similarities\n",
        "    board_df.reset_index(inplace = True)\n",
        "\n",
        "    ## next we find the product of similarities between c-w1 and c-w2 for that specific board's 190 word-pairs\n",
        "    ## this gives us a 190 x N array of product similarities for a given combs_df\n",
        "    ## specifically, for each possible pair, pull out \n",
        "    f_w1_list =  np.array([clue_sims[board_df[board_df[\"Word\"]==row[\"Word1\"]].index.values[0]]\n",
        "                         for  index, row in combs_df.iterrows()])\n",
        "    f_w2_list =  np.array([clue_sims[board_df[board_df[\"Word\"]==row[\"Word2\"]].index.values[0]] \n",
        "                         for  index, row in combs_df.iterrows()])\n",
        "\n",
        "    # result is of length 190 for the product of similarities (i.e. how similar each word i is to BOTH in pair)\n",
        "    # note that cosine is in range [-1, 1] so we have to convert to [0,1] for this conjunction to be valid\n",
        "    return ((f_w1_list + 1) /2) * ((f_w2_list + 1)/2)\n",
        "\n",
        "board_matrices = {\n",
        "    key : {board_name : create_board_matrix(board_combos[board_name], boards[board_name], embedding) \n",
        "           for board_name in boards.keys()}\n",
        "    for (key, embedding) in representations.items()\n",
        "}\n",
        "\n",
        "def literal_guesser_np(board_name, representation):\n",
        "    boardmatrix = board_matrices[representation][board_name]\n",
        "    return softmax(boardmatrix, axis=0)\n",
        "\n",
        "def pragmatic_speaker_np(board_name, beta, costweight, representation):\n",
        "    literal_guesser_prob = np.log(literal_guesser_np(board_name, representation))\n",
        "    clues_cost = -np.array(list(sample_df[\"LgSUBTLWF\"]))\n",
        "    utility = (1-costweight) * literal_guesser_prob - costweight * clues_cost\n",
        "    return softmax(beta * utility, axis = 1)\n",
        "\n",
        "## define new \"candidate\" functions\n",
        "\n",
        "def pragmatic_speaker_candidates(board_name, beta, costweight, representation, candidates):\n",
        "    candidate_index = [list(sample_df[\"Word\"]).index(w) for w in candidates]\n",
        "    literal_guesser_prob = np.log(literal_guesser_candidates(board_name, representation, candidates))\n",
        "    clues_cost = -np.array([list(sample_df[\"LgSUBTLWF\"])[i] for i in candidate_index])\n",
        "    utility = (1-costweight) * literal_guesser_prob - costweight * clues_cost\n",
        "    return softmax(beta * utility, axis = 1)\n",
        "\n",
        "def literal_guesser_candidates(board_name, representation, candidates):\n",
        "    boardmatrix = board_matrices[representation][board_name]\n",
        "    ## here we restrict the softmax to only certain candidates\n",
        "    candidate_index = [list(sample_df[\"Word\"]).index(w) for w in candidates]\n",
        "    restricted_boardmatrix = boardmatrix[:,candidate_index]\n",
        "    return softmax(restricted_boardmatrix, axis=0)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c46cGdcHMM2q"
      },
      "source": [
        "## Running an example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QJnrnBGOZ26"
      },
      "source": [
        "threshold = 0.4\n",
        "sim_matrix_glove = create_similarity_matrix(representations['glove'], representations['glove'], representations['glove'].shape[0])\n",
        "Graph = create_graph(sim_matrix_glove, threshold)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4voFxHxOb22",
        "outputId": "3ef64afa-ab8e-4302-dbc6-3475223bf25f"
      },
      "source": [
        "target = 'trauma-weird'\n",
        "w1, w2 = target.split(sep = \"-\")\n",
        "n_steps = 5\n",
        "n_walks = 1000\n",
        "\n",
        "print(f\"example of a random walk of {n_steps} steps from {w1}\",random_walk(w1, n_steps))\n",
        "print(f\"example of a random walk of {n_steps} steps from {w2}\",random_walk(w2, n_steps))\n",
        "\n",
        "# computing union and intersection of independent walks\n",
        "u, i = union_intersection(w1,w2, n_steps, n_walks, vocab)\n",
        "\n",
        "union_candidates = list(u.vocab_word)\n",
        "int_candidates = list(i.vocab_word)\n",
        "\n",
        "print(f\"{len(u)} items in union: {union_candidates[:5]}\")\n",
        "print(f\"{len(i)} items in intersection: {int_candidates[:5]}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example of a random walk of 5 steps from trauma (['trauma', 'mental', 'criminal', 'could', 'age', 'disability'], array([0., 0., 0., ..., 0., 0., 0.]))\n",
            "example of a random walk of 5 steps from weird (['weird', 'sexy', 'dirty', 'boring', 'depressing', 'awful'], array([0., 0., 0., ..., 0., 0., 0.]))\n",
            "3067 items in union: ['horrific', 'experience', 'kind', 'sort', 'something']\n",
            "689 items in intersection: ['horrific', 'experience', 'something', 'kind', 'sort']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFlv28866gwB",
        "outputId": "8fb0dfac-0487-4e0f-eb07-5925359f7850"
      },
      "source": [
        "clue = 'study'\n",
        "n = 5\n",
        "wordpairlist = get_wordpair_list(board_combos, 'e1_board1_words')\n",
        "target_index = wordpairlist.index(target)\n",
        "clue_index = list(sample_df[\"Word\"]).index(clue)\n",
        "\n",
        "a = literal_guesser_np('e1_board1_words', 'glove')[:,clue_index]\n",
        "y = pragmatic_speaker_np('e1_board1_words', 18.858, 0.004, 'glove')\n",
        "\n",
        "b_union = literal_guesser_candidates('e1_board1_words', 'glove', union_candidates)[:, union_candidates.index(clue)]\n",
        "c_union = pragmatic_speaker_candidates('e1_board1_words', 18.858, 0.004, 'glove', union_candidates)\n",
        "\n",
        "b_int = literal_guesser_candidates('e1_board1_words', 'glove', int_candidates)[:, int_candidates.index(clue)]\n",
        "c_int = pragmatic_speaker_candidates('e1_board1_words', 18.858, 0.004, 'glove', int_candidates)\n",
        "\n",
        "\n",
        "print(f\"for wordpair {target} and clue {clue}\")\n",
        "\n",
        "print(f\"considering literal guesser predictions....\")\n",
        "\n",
        "print(\"literal guesser prediction is:\", wordpairlist[np.argmax(a)])\n",
        "print(\"literal candidate UNION prediction is:\", wordpairlist[np.argmax(b_union)])\n",
        "print(\"literal candidate INTERSECTION prediction is:\", wordpairlist[np.argmax(b_int)])\n",
        "\n",
        "print(f\"considering pragmatic guesser predictions....\")\n",
        "\n",
        "top = y[target_index,:].argsort()[-5:][::-1].tolist()\n",
        "top_words = [list(sample_df[\"Word\"])[x] for x in top]\n",
        "print(f\"top {n} prag speaker predictions are:\", top_words)\n",
        "\n",
        "top = c_union[target_index,:].argsort()[-5:][::-1].tolist()\n",
        "top_words = [list(u.vocab_word)[x] for x in top]\n",
        "print(f\"top {n} prag candidate UNION are:\", top_words)\n",
        "\n",
        "top = c_int[target_index,:].argsort()[-5:][::-1].tolist()\n",
        "top_words = [list(i.vocab_word)[x] for x in top]\n",
        "print(f\"top {n} prag candidate INTERSECTION are:\", top_words)\n",
        "\n",
        "print(f\"considering top {n} nodes visited by union and intersection...\")\n",
        "print(f\"highly visited nodes in the union: {union_candidates[:n]}\")\n",
        "print(f\"highly visited nodes in the intersection: {int_candidates[:n]}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for wordpair exam-algebra and clue study\n",
            "considering literal guesser predictions....\n",
            "literal guesser prediction is: write-better\n",
            "literal candidate UNION prediction is: write-better\n",
            "literal candidate INTERSECTION prediction is: write-better\n",
            "considering pragmatic guesser predictions....\n",
            "top 5 prag speaker predictions are: ['algebra', 'exam', 'mathematics', 'exams', 'calculus']\n",
            "top 5 prag candidate UNION are: ['algebra', 'exam', 'mathematics', 'exams', 'calculus']\n",
            "top 5 prag candidate INTERSECTION are: ['algebra', 'mathematics', 'math', 'grade', 'geometry']\n",
            "considering top 5 nodes visited by union and intersection...\n",
            "highly visited nodes in the union: ['mathematics', 'algebra', 'mathematical', 'arithmetic', 'finite']\n",
            "highly visited nodes in the intersection: ['mathematics', 'algebra', 'mathematical', 'arithmetic', 'finite']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cqRx40xQuKS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}