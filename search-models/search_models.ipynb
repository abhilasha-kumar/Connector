{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhilasha-kumar/Connector/blob/master/search-models/search_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH3K3UX0r_uM"
      },
      "source": [
        "# Cloning git repository & load libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/abhilasha-kumar/Connector.git\n",
        "!pip install pybind11\n",
        "!pip install graph-walker # fast random walk implementation from https://github.com/kerighan/graph-walker"
      ],
      "metadata": {
        "id": "RU7It8y3eUZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54ZMyOWfsEGS"
      },
      "source": [
        "# Importing embeddings, vocabulary, & functions file\n",
        "We load the embeddings, vocabulary, and all the search functions. The functions have been predefined and are stored in the search-models subdirectory in the github repository, so we directly load them here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxoEmRAHPuDn"
      },
      "outputs": [],
      "source": [
        "# import glove embeddings\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "%cd /content/Connector/connector-cogsci2021/data\n",
        "representations = {}\n",
        "representations['glove'] = pd.read_csv(\"glove_embeddings.csv\").transpose().values\n",
        "representations['swow'] = pd.read_csv(\"swow_embeddings.csv\").transpose().values\n",
        "vocab = pd.read_csv(\"vocab.csv\").rename(columns={\"Word\": \"vocab_word\"})\n",
        "print(f\"embeddings are shaped:\", representations['glove'].shape)\n",
        "print(f\"vocab is {len(vocab)} words\")\n",
        "with open('boards.json', 'r') as json_file:\n",
        "    boards = json.load(json_file)\n",
        "\n",
        "## import empirical clues (cleaned)\n",
        "expdata = pd.read_csv(\"final_board_clues_all.csv\", encoding= 'unicode_escape')\n",
        "\n",
        "## need to get similarity matrix of these words in this order to work with\n",
        "target_df = pd.read_csv(\"connector_wordpairs_boards.csv\")\n",
        "target_df[\"wordpair\"]= target_df[\"Word1\"]+ \"-\"+target_df[\"Word2\"]\n",
        "print(target_df.head())\n",
        "\n",
        "%cd /content/Connector/search-models\n",
        "import search_funcs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtuHKP7ItXDy"
      },
      "source": [
        "# Running through full dataset\n",
        "\n",
        "Having verified the functions, we will now run these functions on the full behavioral dataset. We will need to explore some parameters for this. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We have 2 variables:\n",
        "1.   Candidates (full/subset)\n",
        "2.   Pragmatics (with/without)\n",
        "\n",
        "We have the following search models that generate candidates:\n",
        "1.   Union (RW)\n",
        "2.   Intersection (RW)\n",
        "3.   Predication (spreading activation)\n",
        "\n",
        "We have the following models that can be run with/without candidates and with/without pragmatics:\n",
        "1.   Speaker models\n",
        "    - Target+Board\n",
        "    - Pragmatic speaker\n",
        "2.  Guesser models \n",
        "  - Literal Guesser\n",
        "  - Pragmatic Guesser\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u0svZpSd88Jz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: On full vocab"
      ],
      "metadata": {
        "id": "Qt6jgRIeM7YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## create boards and merge with expdata\n",
        "combined_boards_df = pd.DataFrame(columns=['Experiment', 'Board','boardwords'])\n",
        "combined_boards_df[\"Experiment\"]  = [\"E1\"] * 10 + [\"E2\"] * 10\n",
        "combined_boards_df[\"Board\"] = [\"TrialList\" + str(i) for i in range(1,11)] * 2\n",
        "combined_boards_df[\"boardnames\"] = (['e1_board' + str(i) + '_words' for i in range(1,11)] \n",
        "                                  + ['e2_board' + str(i) + '_words' for i in range(1,11)])\n",
        "combined_boards_df[\"boardwords\"] = [boards[n] for n in combined_boards_df[\"boardnames\"]]\n",
        "\n",
        "expdata_new = pd.merge(expdata,combined_boards_df,on=['Board', 'Experiment'],how='left')\n",
        "expdata_new[\"wordpair\"] = expdata_new[\"Word1\"] + \"-\" + expdata_new[\"Word2\"]\n",
        "board_combos = {board_name : search_funcs.RSA.compute_board_combos(board_name, boards) for board_name in boards.keys()}"
      ],
      "metadata": {
        "id": "5gKTDYETeKlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Non - RSA method\n"
      ],
      "metadata": {
        "id": "FZOYctgjM5NM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "board_optimal_params = {\n",
        "    'swow' : (23.488850322875496, 1), # -13204\n",
        "    'glove' : (20.952928531665275, 1), # -15774.814774380024)\n",
        "    'bert-sum' : (19.983835225540847, 0.787924454045298),\n",
        "}"
      ],
      "metadata": {
        "id": "v8QEomkF4hmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluescoredf = search_funcs.nonRSA.speaker_targetboard_cluescores(['swow', 'glove'], board_optimal_params, board_combos, boards, list(vocab.vocab_word), vocab, representations, target_df, expdata_new)\n",
        "cluescoredf.head()"
      ],
      "metadata": {
        "id": "9r7kmGrcNCvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Models with RSA"
      ],
      "metadata": {
        "id": "67827P5C897A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rsa_optimal_params = {\n",
        "    'swow' : (25.1522030761838, 0.03863169001849234),\n",
        "    'glove' : (22.336514544537227, 0.039),\n",
        "    'bert-sum' : (29.709602301411962, 0.031659060110267576), #-17533\n",
        "}"
      ],
      "metadata": {
        "id": "IVrhPE-xN6I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pragmaticspeakerdf = search_funcs.RSA.get_speaker_df(representations, combined_boards_df,rsa_optimal_params, list(vocab.vocab_word), vocab, expdata_new, board_combos, target_df, boards)\n",
        "pragmaticspeakerdf.head()"
      ],
      "metadata": {
        "id": "ewq3Y68zSd_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Candidate generation (Union & Intersection)"
      ],
      "metadata": {
        "id": "Ht74xlvhz7xv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlh_haE7tazm"
      },
      "outputs": [],
      "source": [
        "## here we generate candidates for each of our wordpairs: stored in target_df\n",
        "\n",
        "candidates_df = pd.DataFrame()\n",
        "\n",
        "# keep n_walks fixed to a large number\n",
        "n_walks = 1000\n",
        "n_steps = 50\n",
        "\n",
        "## should probably try a range of values for threshold \n",
        "\n",
        "for modelname in ['glove']:\n",
        "  for threshold in np.arange(0.2, 0.3, 0.1):\n",
        "    print(f\"threshold is {threshold}\")\n",
        "    # sim_matrix = search_funcs.search.create_similarity_matrix(representations[modelname])\n",
        "    # Graph = search_funcs.search.create_graph(sim_matrix, threshold)\n",
        "    print(f\"graph has been created\")\n",
        "    for index, row in target_df.iterrows():\n",
        "      w1 = row[\"Word1\"]\n",
        "      w2 = row[\"Word2\"]\n",
        "      print(f\"for {w1} and {w2}\")\n",
        "      union_df, int_df = search_funcs.search.union_intersection(w1,w2, n_steps, n_walks, vocab, Graph)\n",
        "      print(f\"union/int calculation complete!\")\n",
        "      \n",
        "      union_df[\"Word1\"] = w1\n",
        "      union_df[\"Word2\"] = w2\n",
        "      union_df[\"representation\"] = modelname\n",
        "      union_df[\"threshold\"] = threshold\n",
        "      union_df[\"n_walks\"] = n_walks\n",
        "      union_df[\"type\"] = \"union\"\n",
        "      \n",
        "\n",
        "      int_df[\"Word1\"] = w1\n",
        "      int_df[\"Word2\"] = w2\n",
        "      int_df[\"representation\"] = modelname\n",
        "      int_df[\"threshold\"] = threshold\n",
        "      int_df[\"n_walks\"] = n_walks\n",
        "      int_df[\"type\"] = \"intersection\"\n",
        "\n",
        "      overall_df = pd.concat([union_df, int_df])\n",
        "\n",
        "      candidates_df = pd.concat([candidates_df, overall_df])\n",
        "\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing files to CSV"
      ],
      "metadata": {
        "id": "iuxZRpUSYSyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "id": "ALztsNDTyVEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parentfolder = \"/content/drive/My Drive/search-models/\"\n",
        "pragmaticspeakerdf.to_csv(parentfolder+'pragmaticspeakerdf.csv')"
      ],
      "metadata": {
        "id": "woMZx6ojybl5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "search-models.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPhiSSgwdAY2MmRZ2DDCNLY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}