{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhilasha-kumar/Connector/blob/master/search-models/search_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH3K3UX0r_uM"
      },
      "source": [
        "# Cloning git repository & load libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/abhilasha-kumar/Connector.git\n",
        "!pip install pybind11\n",
        "!pip install graph-walker # fast random walk implementation from https://github.com/kerighan/graph-walker"
      ],
      "metadata": {
        "id": "RU7It8y3eUZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54ZMyOWfsEGS"
      },
      "source": [
        "# Importing embeddings, vocabulary, & functions file\n",
        "We load the embeddings, vocabulary, and all the search functions. The functions have been predefined and are stored in the search-models subdirectory in the github repository, so we directly load them here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxoEmRAHPuDn"
      },
      "outputs": [],
      "source": [
        "# import glove embeddings\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "%cd /content/Connector/connector-cogsci2021/data\n",
        "representations = {}\n",
        "representations['glove'] = pd.read_csv(\"glove_embeddings.csv\").transpose().values\n",
        "representations['swow'] = pd.read_csv(\"swow_embeddings.csv\").transpose().values\n",
        "vocab = pd.read_csv(\"vocab.csv\").rename(columns={\"Word\": \"vocab_word\"})\n",
        "print(f\"embeddings are shaped:\", representations['glove'].shape)\n",
        "print(f\"vocab is {len(vocab)} words\")\n",
        "with open('boards.json', 'r') as json_file:\n",
        "    boards = json.load(json_file)\n",
        "\n",
        "## import empirical clues (cleaned)\n",
        "expdata = pd.read_csv(\"final_board_clues_all.csv\", encoding= 'unicode_escape')\n",
        "\n",
        "## need to get similarity matrix of these words in this order to work with\n",
        "target_df = pd.read_csv(\"connector_wordpairs_boards.csv\")\n",
        "target_df[\"wordpair\"]= target_df[\"Word1\"]+ \"-\"+target_df[\"Word2\"]\n",
        "print(target_df.head())\n",
        "\n",
        "%cd /content/Connector/search-models\n",
        "import search_funcs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtuHKP7ItXDy"
      },
      "source": [
        "# Running through full dataset\n",
        "\n",
        "Having verified the functions, we will now run these functions on the full behavioral dataset. We will need to explore some parameters for this. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We have 2 variables:\n",
        "1.   Candidates (full/subset)\n",
        "2.   Pragmatics (with/without)\n",
        "\n",
        "We have the following search models that generate candidates:\n",
        "1.   Union (RW)\n",
        "2.   Intersection (RW)\n",
        "3.   Predication (spreading activation)\n",
        "\n",
        "We have the following models that can be run with/without candidates and with/without pragmatics:\n",
        "1.   Speaker models\n",
        "    - Target+Board\n",
        "    - Pragmatic speaker\n",
        "2.  Guesser models \n",
        "  - Literal Guesser\n",
        "  - Pragmatic Guesser\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u0svZpSd88Jz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: On full vocab"
      ],
      "metadata": {
        "id": "Qt6jgRIeM7YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## create boards and merge with expdata\n",
        "combined_boards_df = pd.DataFrame(columns=['Experiment', 'Board','boardwords'])\n",
        "combined_boards_df[\"Experiment\"]  = [\"E1\"] * 10 + [\"E2\"] * 10\n",
        "combined_boards_df[\"Board\"] = [\"TrialList\" + str(i) for i in range(1,11)] * 2\n",
        "combined_boards_df[\"boardnames\"] = (['e1_board' + str(i) + '_words' for i in range(1,11)] \n",
        "                                  + ['e2_board' + str(i) + '_words' for i in range(1,11)])\n",
        "combined_boards_df[\"boardwords\"] = [boards[n] for n in combined_boards_df[\"boardnames\"]]\n",
        "\n",
        "expdata_new = pd.merge(expdata,combined_boards_df,on=['Board', 'Experiment'],how='left')\n",
        "expdata_new[\"wordpair\"] = expdata_new[\"Word1\"] + \"-\" + expdata_new[\"Word2\"]\n",
        "board_combos = {board_name : search_funcs.RSA.compute_board_combos(board_name, boards) for board_name in boards.keys()}"
      ],
      "metadata": {
        "id": "5gKTDYETeKlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Non - RSA method\n"
      ],
      "metadata": {
        "id": "FZOYctgjM5NM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "board_optimal_params = {\n",
        "    'swow' : (23.488850322875496, 1), # -13204\n",
        "    'glove' : (20.952928531665275, 1), # -15774.814774380024)\n",
        "    'bert-sum' : (19.983835225540847, 0.787924454045298),\n",
        "}"
      ],
      "metadata": {
        "id": "v8QEomkF4hmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluescoredf = search_funcs.nonRSA.speaker_targetboard_cluescores(['swow', 'glove'], board_optimal_params, board_combos, boards, list(vocab.vocab_word), vocab, representations, target_df, expdata_new)\n",
        "cluescoredf.head()"
      ],
      "metadata": {
        "id": "9r7kmGrcNCvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Models with RSA"
      ],
      "metadata": {
        "id": "67827P5C897A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rsa_optimal_params = {\n",
        "    'swow' : (25.1522030761838, 0.03863169001849234),\n",
        "    'glove' : (22.336514544537227, 0.039),\n",
        "    'bert-sum' : (29.709602301411962, 0.031659060110267576), #-17533\n",
        "}"
      ],
      "metadata": {
        "id": "IVrhPE-xN6I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pragmaticspeakerdf = search_funcs.RSA.get_speaker_df(representations, combined_boards_df,rsa_optimal_params, list(vocab.vocab_word), vocab, expdata_new, board_combos, target_df, boards)\n",
        "pragmaticspeakerdf.head()"
      ],
      "metadata": {
        "id": "ewq3Y68zSd_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pragmaticspeaker_df"
      ],
      "metadata": {
        "id": "NU76eTFKFUQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Candidate generation (Union & Intersection)"
      ],
      "metadata": {
        "id": "Ht74xlvhz7xv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlh_haE7tazm"
      },
      "outputs": [],
      "source": [
        "## here we generate candidates for each of our wordpairs: stored in target_df\n",
        "\n",
        "candidates_df = pd.DataFrame()\n",
        "\n",
        "# keep n_walks fixed to a large number\n",
        "n_walks = 1000\n",
        "n_steps = 50\n",
        "\n",
        "## should probably try a range of values for threshold \n",
        "\n",
        "for modelname in ['glove']:\n",
        "  for threshold in np.arange(0.2, 0.3, 0.1):\n",
        "    print(f\"threshold is {threshold}\")\n",
        "    # sim_matrix = search_funcs.search.create_similarity_matrix(representations[modelname])\n",
        "    # Graph = search_funcs.search.create_graph(sim_matrix, threshold)\n",
        "    print(f\"graph has been created\")\n",
        "    for index, row in target_df.iterrows():\n",
        "      w1 = row[\"Word1\"]\n",
        "      w2 = row[\"Word2\"]\n",
        "      print(f\"for {w1} and {w2}\")\n",
        "      union_df, int_df = search_funcs.search.union_intersection(w1,w2, n_steps, n_walks, vocab, Graph)\n",
        "      print(f\"union/int calculation complete!\")\n",
        "      \n",
        "      union_df[\"Word1\"] = w1\n",
        "      union_df[\"Word2\"] = w2\n",
        "      union_df[\"representation\"] = modelname\n",
        "      union_df[\"threshold\"] = threshold\n",
        "      union_df[\"n_walks\"] = n_walks\n",
        "      union_df[\"type\"] = \"union\"\n",
        "      \n",
        "\n",
        "      int_df[\"Word1\"] = w1\n",
        "      int_df[\"Word2\"] = w2\n",
        "      int_df[\"representation\"] = modelname\n",
        "      int_df[\"threshold\"] = threshold\n",
        "      int_df[\"n_walks\"] = n_walks\n",
        "      int_df[\"type\"] = \"intersection\"\n",
        "\n",
        "      overall_df = pd.concat([union_df, int_df])\n",
        "\n",
        "      candidates_df = pd.concat([candidates_df, overall_df])\n",
        "\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "candidates_df = pd.read_csv(parentfolder+'swow_candidates.csv')\n",
        "candidates_df"
      ],
      "metadata": {
        "id": "tqvJhhc5Gw5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = candidates_df[(candidates_df['vocab_word'] != candidates_df[\"Word1\"]) & (candidates_df['vocab_word'] != candidates_df[\"Word2\"])]\n",
        "\n",
        "df_filtered[\"wordpair\"] = df_filtered[\"Word1\"] + \"-\"+df_filtered[\"Word2\"]\n",
        "## group by number of steps in the RW and union/intersection\n",
        "clong = df_filtered.groupby(['wordpair', 'type', 'n_steps'], as_index=False)['vocab_word'].agg(','.join)\n",
        "clong['clue_list'] = clong['vocab_word'].str.split(',')\n",
        "clong = clong.merge(target_df, on = \"wordpair\")\n",
        "clong"
      ],
      "metadata": {
        "id": "JUEhSzKwGbBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RSA"
      ],
      "metadata": {
        "id": "dQiVTd1wDb-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rsa_optimal_params = {\n",
        "    'swow' : (25.1522030761838, 0.03863169001849234),\n",
        "    'glove' : (82.83019661384789, 0.9997249702731884),\n",
        "    'bert-sum' : (29.709602301411962, 0.031659060110267576), #-17533\n",
        "}"
      ],
      "metadata": {
        "id": "eH-KcyesGuJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parentfolder = \"/content/drive/My Drive/search-models/\"\n",
        "pragmaticspeaker_df = pd.read_csv(parentfolder+'candidates_RSAprobs.csv')\n"
      ],
      "metadata": {
        "id": "scM-hPvIKUSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## need to obtain list of candidates for each board separately\n",
        "\n",
        "#pragmaticspeaker_df = pd.DataFrame()\n",
        "modelname = 'swow'\n",
        "\n",
        "beta = rsa_optimal_params[modelname][0]\n",
        "cost = rsa_optimal_params[modelname][1]\n",
        "for index, row in clong[1278:].iterrows():\n",
        "  boardname = row[\"boardnames\"]\n",
        "  cluelist = row[\"clue_list\"]\n",
        "  wordpair = row[\"wordpair\"]\n",
        "  clue_probs = search_funcs.RSA.pragmatic_speaker(boardname, beta, cost, representations, 'swow', cluelist, vocab, boards)\n",
        "  ## obtain the probs for the specific wordpair\n",
        "  combos_df = search_funcs.RSA.compute_board_combos(boardname,boards)\n",
        "  wordpairlist = list(combos_df[\"wordpair\"])\n",
        "  mainscores = clue_probs[wordpairlist.index(wordpair)]\n",
        "\n",
        "  clue_board_df = pd.DataFrame({'Model': [modelname]})\n",
        "  clue_board_df[\"boardnames\"] = boardname\n",
        "  clue_board_df[\"type\"] = row[\"type\"]\n",
        "  clue_board_df[\"n_steps\"] = row[\"n_steps\"]   \n",
        "  clue_board_df[\"wordpair\"] = wordpair\n",
        "  clue_board_df[\"cluelist\"] = str(','.join(cluelist))\n",
        "  clue_board_df[\"clue_score\"] = str(np.round(mainscores,10).tolist())\n",
        "    \n",
        "  pragmaticspeaker_df = pd.concat([pragmaticspeaker_df, clue_board_df])\n",
        "  pragmaticspeaker_df.to_csv(parentfolder+'candidates_RSAprobs.csv', index = False)"
      ],
      "metadata": {
        "id": "DlsUEOyaGgrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### obtaining probabilities for data\n",
        "\n",
        "Now that we have the candidate-level probabilities for the pragmatic speaker, we compute the probabilities for the behavioral data"
      ],
      "metadata": {
        "id": "iiRgjHRjDgY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#candidates_df[\"wordpair\"] = candidates_df[\"Word1\"]+\"-\"+candidates_df[\"Word2\"]\n",
        "\n",
        "#rsa_probs = pd.DataFrame()\n",
        "\n",
        "for index, row in expdata[518:].iterrows():\n",
        "  wordpair = row[\"wordpair\"].replace(\" - \", \"-\")\n",
        "  clue = row[\"Clue1\"]\n",
        "  clue_df = df_filtered[(df_filtered[\"vocab_word\"] == clue) & (df_filtered[\"wordpair\"] == wordpair)]\n",
        "  for i, j in clue_df.iterrows():\n",
        "    ctype = j[\"type\"]\n",
        "    n_steps = j[\"n_steps\"]\n",
        "    clueprobs_df = pragmaticspeaker_df[(pragmaticspeaker_df[\"wordpair\"]== wordpair) & (pragmaticspeaker_df[\"type\"] == ctype) & (pragmaticspeaker_df[\"n_steps\"]==n_steps)]\n",
        "    if(len(clueprobs_df)>0):\n",
        "      clue_index = list(clueprobs_df.cluelist)[0].split(',').index(clue)\n",
        "      clue_score = list(clueprobs_df.clue_score)[0][1:-1].split(', ')[clue_index]\n",
        "\n",
        "      clue_board_df = pd.DataFrame({'alpha': [\"RSA\"]})\n",
        "      clue_board_df[\"type\"] = ctype\n",
        "      clue_board_df[\"n_steps\"] = n_steps\n",
        "      clue_board_df[\"wordpair\"] = wordpair\n",
        "      clue_board_df[\"Clue1\"] = clue\n",
        "      clue_board_df[\"clue_score\"] = clue_score\n",
        "        \n",
        "      rsa_probs = pd.concat([rsa_probs, clue_board_df])\n",
        "      rsa_probs.to_csv(parentfolder+'finalexpdata_RSA_speaker.csv', index = False)"
      ],
      "metadata": {
        "id": "Z0pBTT6DFdit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rsa_probs.to_csv(parentfolder+'finalexpdata_RSA_speaker.csv', index = False)"
      ],
      "metadata": {
        "id": "rOZASDhhNiZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing with online candidates study"
      ],
      "metadata": {
        "id": "4zOJOeWyc2XF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parentfolder = \"/content/drive/My Drive/search-models/\"\n",
        "online = pd.read_csv(parentfolder+'online_new_coded.csv')"
      ],
      "metadata": {
        "id": "oZVIHwIkc4zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_online = online[[\"wordpair_id\", \"Level\", \"clueOption1\", \"clueOption2\", \"clueOption3\", \"clueOption4\", \"clueOption5\", \"clueOption6\", \"clueOption7\", \"clueOption8\"]]\n",
        "main_online = main_online.melt(id_vars=['wordpair_id', 'Level'], value_vars=[\"clueOption1\", \"clueOption2\", \"clueOption3\", \"clueOption4\", \"clueOption5\", \"clueOption6\", \"clueOption7\", \"clueOption8\"])\n",
        "main_online = main_online.dropna()\n",
        "main_online = main_online.drop_duplicates()\n",
        "main_online = main_online.groupby(['wordpair_id', 'variable', 'Level'], as_index=False)['value'].agg(','.join)\n",
        "main_online['clue_list'] = main_online['value'].str.split(',')"
      ],
      "metadata": {
        "id": "kQpRbWg4eSSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_candidates = pd.DataFrame()\n",
        "\n",
        "for index, row in main_online.iterrows():\n",
        "  wordpair = row[\"wordpair_id\"]\n",
        "  clue_list = row[\"clue_list\"]\n",
        "  # find all candidates for that wordpair in clong\n",
        "  wp_candidates = clong[clong[\"wordpair\"] == wordpair]\n",
        "  if(len(wp_candidates) == 0):\n",
        "    w1, w2 = wordpair.split(\"-\")\n",
        "    wordpair = w2 + \"-\" + w1\n",
        "    wp_candidates = clong[clong[\"wordpair\"] == wordpair]\n",
        "    \n",
        "  for i, j in wp_candidates.iterrows():\n",
        "    candidate_list = j[\"clue_list\"]\n",
        "    intersection = list(set(clue_list).intersection(candidate_list))\n",
        "\n",
        "    clue_board_df = pd.DataFrame({'wordpair': [wordpair]})\n",
        "    clue_board_df[\"Level\"] = row[\"Level\"]\n",
        "    clue_board_df[\"candidate_type\"] = row[\"variable\"]\n",
        "    clue_board_df[\"type\"] = j[\"type\"]\n",
        "    clue_board_df[\"n_steps\"] = j[\"n_steps\"]\n",
        "    clue_board_df[\"n_model_candidates\"] = len(candidate_list)\n",
        "    clue_board_df[\"n_human_candidates\"] = len(clue_list)\n",
        "    clue_board_df[\"n_intersection\"] = len(intersection)\n",
        "    clue_board_df[\"intersection\"] = str(intersection)\n",
        "      \n",
        "    common_candidates = pd.concat([common_candidates, clue_board_df])\n",
        "\n",
        "common_candidates.to_csv(parentfolder+'common_candidates.csv', index = False)"
      ],
      "metadata": {
        "id": "kox0BAcEfAbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing files to CSV"
      ],
      "metadata": {
        "id": "iuxZRpUSYSyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "id": "ALztsNDTyVEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parentfolder = \"/content/drive/My Drive/search-models/\"\n",
        "common_candidates.to_csv(parentfolder+'common_candidates.csv', index = False)"
      ],
      "metadata": {
        "id": "woMZx6ojybl5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "search-models.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMOyxl21R4ywnAf2K5c9Luj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}