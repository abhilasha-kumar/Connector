{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "killing-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "private-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "timely-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "controlled-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocab = pd.read_csv(\"vocab.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "organized-prompt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vocab[\"Word\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hybrid-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"[CLS] \" + df_vocab[\"Word\"][0] + \" [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "controlled-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_text = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "accurate-sympathy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'a', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "recognized-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenizer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "willing-parks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1037, 102]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "center-cleaners",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[1037]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "labeled-connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 101\n",
      "a 1037\n",
      "[SEP] 102\n"
     ]
    }
   ],
   "source": [
    "for text, index in zip(tokenizer_text, indexed_tokens):\n",
    "    print(text,index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "strange-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_ids = [1] * len(tokenizer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "local-onion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "under-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "arbitrary-transfer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "automotive-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "decreased-appeal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "          [ 0.5956,  0.5420,  0.0412,  ...,  0.4376,  0.5639,  0.3365],\n",
       "          [-0.4815, -0.0189,  0.0092,  ..., -0.2806,  0.3895, -0.2815]]]),\n",
       " tensor([[[ 0.1383, -0.0017, -0.1345,  ...,  0.0213, -0.0027,  0.0546],\n",
       "          [ 0.7541,  0.5487,  0.1515,  ...,  0.2371,  0.1910,  0.0636],\n",
       "          [-0.3558,  0.3080, -0.1537,  ..., -0.4325,  0.8373, -0.1724]]]),\n",
       " tensor([[[-0.0382, -0.1733, -0.1631,  ...,  0.0534,  0.0795,  0.0761],\n",
       "          [ 0.3919,  0.5963,  0.4106,  ...,  0.5559,  0.2573,  0.3806],\n",
       "          [-0.3174,  0.0915,  0.0514,  ..., -0.2584,  0.7475, -0.0926]]]),\n",
       " tensor([[[ 0.0025, -0.2923, -0.0254,  ...,  0.2050,  0.1381,  0.2798],\n",
       "          [ 0.2570,  0.4708,  0.6375,  ...,  0.5920,  0.1055, -0.1956],\n",
       "          [-0.0878, -0.0809,  0.0926,  ..., -0.0043,  0.1657,  0.0037]]]),\n",
       " tensor([[[ 0.0535, -0.4204, -0.5296,  ...,  0.3501, -0.0072,  0.6119],\n",
       "          [ 0.1004,  0.4128, -0.0307,  ..., -0.1951, -0.0311, -0.2469],\n",
       "          [-0.0349, -0.0409,  0.0159,  ..., -0.0076,  0.0641, -0.0299]]]),\n",
       " tensor([[[-0.5462, -0.4105, -0.6200,  ..., -0.1005,  0.1239,  0.7218],\n",
       "          [-0.0299,  0.3036, -0.1488,  ..., -0.5726, -0.5566, -0.4958],\n",
       "          [-0.0174, -0.0346,  0.0222,  ...,  0.0207,  0.0106, -0.0453]]]),\n",
       " tensor([[[-0.9058, -0.9778, -0.6808,  ..., -0.4224,  0.2937,  0.6470],\n",
       "          [-0.2821,  0.1633, -0.0692,  ..., -0.8020,  0.0366, -0.8279],\n",
       "          [ 0.0033, -0.0395, -0.0051,  ...,  0.0044, -0.0143, -0.0405]]]),\n",
       " tensor([[[-1.4268, -0.5541, -1.1470,  ..., -0.0106,  0.1230,  1.2595],\n",
       "          [-0.6685, -0.4801, -1.0215,  ..., -0.4477,  0.2517, -0.2407],\n",
       "          [-0.0281, -0.0652, -0.0086,  ..., -0.0074,  0.0132, -0.0600]]]),\n",
       " tensor([[[-1.5504, -0.5364, -1.2643,  ..., -0.6518, -0.0025,  1.0714],\n",
       "          [-0.8516, -0.2341, -0.7218,  ..., -0.4597, -0.1297, -0.4168],\n",
       "          [-0.0183, -0.0542,  0.0022,  ..., -0.0496, -0.0302, -0.0870]]]),\n",
       " tensor([[[-1.7478, -0.7176, -1.4446,  ..., -0.0459,  0.3797,  0.8772],\n",
       "          [-1.2437, -0.1891, -0.6820,  ..., -0.3134,  0.2850, -0.4985],\n",
       "          [-0.0146, -0.0219,  0.0043,  ..., -0.0803, -0.0641, -0.0191]]]),\n",
       " tensor([[[-1.6239, -0.7750, -1.1219,  ..., -0.1473,  0.2898,  0.7232],\n",
       "          [-1.0256, -0.3892, -0.4098,  ..., -0.3498,  0.3374, -0.3997],\n",
       "          [ 0.0065, -0.0063, -0.1165,  ...,  0.0635, -0.0688, -0.0159]]]),\n",
       " tensor([[[-1.3619, -0.6809, -0.5387,  ..., -0.1404,  0.1658,  0.6824],\n",
       "          [-1.0779, -0.8755,  0.0053,  ..., -0.2874,  0.5010, -0.1286],\n",
       "          [ 0.0595,  0.0124, -0.0260,  ...,  0.0135, -0.0466,  0.0223]]]),\n",
       " tensor([[[-0.4422, -0.0526, -0.0091,  ...,  0.0963,  0.1625,  0.3031],\n",
       "          [-0.4922, -0.6787,  0.0211,  ...,  0.2248,  0.6457, -0.2831],\n",
       "          [ 1.0238,  0.0883, -0.3741,  ...,  0.3148, -0.8268, -0.2171]]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "extraordinary-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vec = []\n",
    "sum_vec = []\n",
    "for index, row in df_vocab.iterrows():\n",
    "    text = row[\"Word\"]\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    \n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    \n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(tokens_tensor, segments_tensors)  \n",
    "        hidden_states = outputs[2]\n",
    "        \n",
    "        token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "        \n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        \n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "        \n",
    "    token_vecs_cat = [torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0) for token in token_embeddings]\n",
    "    \n",
    "    token_vecs_sum = [torch.sum(token[-4:], dim=0) for token in token_embeddings]\n",
    "    \n",
    "    main_cat_vec = token_vecs_cat[-2]\n",
    "    \n",
    "    main_sum_vec = token_vecs_sum[-2]\n",
    "    \n",
    "    cat_vec.append(main_cat_vec)\n",
    "    sum_vec.append(main_sum_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "decimal-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_name_1 = \"cat_vec.pkl\"\n",
    "file_name_2 = \"sum_vec.pkl\"\n",
    "open_file_1 = open(file_name_1, \"wb\")\n",
    "open_file_2 = open(file_name_2, \"wb\")\n",
    "pickle.dump(cat_vec, open_file_1)\n",
    "pickle.dump(sum_vec, open_file_2)\n",
    "open_file_1.close()\n",
    "open_file_2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "round-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_pair = pd.read_csv(\"connector_wordpairs_boards.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fresh-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cat_diff = 10000000000.0\n",
    "min_sum_diff = 10000000000.0\n",
    "similar_word_indices_cat = []\n",
    "similar_word_indices_sum = []\n",
    "for index,row in df_word_pair.iterrows():\n",
    "    word1, word2 = row[\"Word1\"],row[\"Word2\"]\n",
    "    avg_cat = torch.divide(torch.add(cat_vec[list(df_vocab[\"Word\"]).index(word1) + 1],cat_vec[list(df_vocab[\"Word\"]).index(word2) + 1]),2)\n",
    "    avg_sum = torch.divide(torch.add(sum_vec[list(df_vocab[\"Word\"]).index(word1) + 1],sum_vec[list(df_vocab[\"Word\"]).index(word2) + 1]),2)\n",
    "    for idx, (word_cat, word_sum) in enumerate(zip(cat_vec, sum_vec)):\n",
    "        if torch.abs(cat_vec[idx] - avg_cat).sum().item() < min_cat_diff:\n",
    "            min_cat_diff = torch.abs(cat_vec[idx] - avg_cat).sum().item()\n",
    "            min_cat_idx = idx\n",
    "        elif torch.abs(sum_vec[idx] - avg_sum).sum().item() < min_sum_diff:\n",
    "            min_sum_diff = torch.abs(sum_vec[idx] - avg_sum).sum().item()\n",
    "            min_sum_idx = idx\n",
    "    similar_word_indices_cat.append(min_cat_idx)\n",
    "    similar_word_indices_sum.append(min_sum_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "third-wiring",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words_cat = []\n",
    "similar_words_sum = []\n",
    "for idx1,idx2 in zip(similar_word_indices_cat, similar_word_indices_sum):\n",
    "    similar_words_cat.append(df_vocab[\"Word\"][idx1])\n",
    "    similar_words_sum.append(df_vocab[\"Word\"][idx2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "moderate-agency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cougar',\n",
       " 'cougar',\n",
       " 'algorithm',\n",
       " 'algorithm',\n",
       " 'algorithm',\n",
       " 'algorithm',\n",
       " 'algorithm',\n",
       " 'algorithm',\n",
       " 'algorithm',\n",
       " 'algorithm',\n",
       " 'algorithm',\n",
       " 'algorithm',\n",
       " 'algorithm',\n",
       " 'algorithm',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'eggnog',\n",
       " 'Easter',\n",
       " 'Easter',\n",
       " 'Easter',\n",
       " 'Easter',\n",
       " 'Easter',\n",
       " 'Easter',\n",
       " 'Easter',\n",
       " 'Easter',\n",
       " 'Easter']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "exclusive-investor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['volcano',\n",
       " 'volcano',\n",
       " 'examination',\n",
       " 'examination',\n",
       " 'examination',\n",
       " 'examination',\n",
       " 'examination',\n",
       " 'examination',\n",
       " 'examination',\n",
       " 'examination',\n",
       " 'examination',\n",
       " 'examination',\n",
       " 'examination',\n",
       " 'examination',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'kite',\n",
       " 'shortage',\n",
       " 'shortage',\n",
       " 'shortage',\n",
       " 'shortage',\n",
       " 'shortage',\n",
       " 'shortage',\n",
       " 'shortage',\n",
       " 'shortage',\n",
       " 'shortage']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "revolutionary-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_pair[\"Similar Word Cat\"] = similar_words_cat\n",
    "df_word_pair[\"Similar Word Sum\"] = similar_words_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "received-egyptian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word1</th>\n",
       "      <th>Word2</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>boardnames</th>\n",
       "      <th>Similar Word Cat</th>\n",
       "      <th>Similar Word Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>void</td>\n",
       "      <td>couch</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board1_words</td>\n",
       "      <td>cougar</td>\n",
       "      <td>volcano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>giggle</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board1_words</td>\n",
       "      <td>cougar</td>\n",
       "      <td>volcano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exam</td>\n",
       "      <td>algebra</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board1_words</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>examination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tea</td>\n",
       "      <td>bean</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board10_words</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>examination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tourist</td>\n",
       "      <td>comedy</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board10_words</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>examination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pendulum</td>\n",
       "      <td>dusk</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board10_words</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>examination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>beginning</td>\n",
       "      <td>brake</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board2_words</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>examination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>birds</td>\n",
       "      <td>aircraft</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board2_words</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>examination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>school</td>\n",
       "      <td>stop</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board2_words</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>examination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>circle</td>\n",
       "      <td>dance</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board3_words</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>examination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>famine</td>\n",
       "      <td>calorie</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board3_words</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>examination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hundred</td>\n",
       "      <td>economy</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board3_words</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>examination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>corpse</td>\n",
       "      <td>fight</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board4_words</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>examination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>graph</td>\n",
       "      <td>copy</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board4_words</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>examination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>egg</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board4_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>near</td>\n",
       "      <td>astronaut</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board5_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>homeless</td>\n",
       "      <td>apartment</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board5_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>onion</td>\n",
       "      <td>cigarette</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board5_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pants</td>\n",
       "      <td>collar</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board6_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>definition</td>\n",
       "      <td>assist</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board6_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>war</td>\n",
       "      <td>quiet</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board6_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>flat</td>\n",
       "      <td>alike</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board7_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>perjury</td>\n",
       "      <td>adultery</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board7_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>heavy</td>\n",
       "      <td>feather</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board7_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>breeze</td>\n",
       "      <td>bubble</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board8_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bad</td>\n",
       "      <td>actress</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board8_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>communicate</td>\n",
       "      <td>cooking</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board8_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bat</td>\n",
       "      <td>bounce</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board9_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>toes</td>\n",
       "      <td>Dracula</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board9_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lunch</td>\n",
       "      <td>almond</td>\n",
       "      <td>E1</td>\n",
       "      <td>e1_board9_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>cave</td>\n",
       "      <td>knight</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board1_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>quick</td>\n",
       "      <td>glow</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board1_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>tree</td>\n",
       "      <td>oak</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board1_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>garage</td>\n",
       "      <td>bone</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board10_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>feet</td>\n",
       "      <td>chapel</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board10_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>chair</td>\n",
       "      <td>table</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board10_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>stern</td>\n",
       "      <td>wind</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board2_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>sun</td>\n",
       "      <td>bowl</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board2_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>gold</td>\n",
       "      <td>silver</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board2_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>holy</td>\n",
       "      <td>kind</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board3_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>teeth</td>\n",
       "      <td>gums</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board3_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>rude</td>\n",
       "      <td>regret</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board3_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>dream</td>\n",
       "      <td>bet</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board4_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>jump</td>\n",
       "      <td>leap</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board4_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>travel</td>\n",
       "      <td>ankle</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board4_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>snake</td>\n",
       "      <td>ash</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board5_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>hand</td>\n",
       "      <td>birth</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board5_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>lion</td>\n",
       "      <td>tiger</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board5_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>elm</td>\n",
       "      <td>rock</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board6_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>crust</td>\n",
       "      <td>boot</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board6_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>sit</td>\n",
       "      <td>stand</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board6_words</td>\n",
       "      <td>eggnog</td>\n",
       "      <td>kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>east</td>\n",
       "      <td>short</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board7_words</td>\n",
       "      <td>Easter</td>\n",
       "      <td>shortage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>old</td>\n",
       "      <td>new</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board7_words</td>\n",
       "      <td>Easter</td>\n",
       "      <td>shortage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>glass</td>\n",
       "      <td>cage</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board7_words</td>\n",
       "      <td>Easter</td>\n",
       "      <td>shortage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>giant</td>\n",
       "      <td>subtle</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board8_words</td>\n",
       "      <td>Easter</td>\n",
       "      <td>shortage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>army</td>\n",
       "      <td>drum</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board8_words</td>\n",
       "      <td>Easter</td>\n",
       "      <td>shortage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>happy</td>\n",
       "      <td>sad</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board8_words</td>\n",
       "      <td>Easter</td>\n",
       "      <td>shortage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>weird</td>\n",
       "      <td>trauma</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board9_words</td>\n",
       "      <td>Easter</td>\n",
       "      <td>shortage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>olive</td>\n",
       "      <td>real</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board9_words</td>\n",
       "      <td>Easter</td>\n",
       "      <td>shortage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>candle</td>\n",
       "      <td>wick</td>\n",
       "      <td>E2</td>\n",
       "      <td>e2_board9_words</td>\n",
       "      <td>Easter</td>\n",
       "      <td>shortage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word1      Word2 Experiment        boardnames Similar Word Cat  \\\n",
       "0          void      couch         E1   e1_board1_words           cougar   \n",
       "1        giggle   abnormal         E1   e1_board1_words           cougar   \n",
       "2          exam    algebra         E1   e1_board1_words        algorithm   \n",
       "3           tea       bean         E1  e1_board10_words        algorithm   \n",
       "4       tourist     comedy         E1  e1_board10_words        algorithm   \n",
       "5      pendulum       dusk         E1  e1_board10_words        algorithm   \n",
       "6     beginning      brake         E1   e1_board2_words        algorithm   \n",
       "7         birds   aircraft         E1   e1_board2_words        algorithm   \n",
       "8        school       stop         E1   e1_board2_words        algorithm   \n",
       "9        circle      dance         E1   e1_board3_words        algorithm   \n",
       "10       famine    calorie         E1   e1_board3_words        algorithm   \n",
       "11      hundred    economy         E1   e1_board3_words        algorithm   \n",
       "12       corpse      fight         E1   e1_board4_words        algorithm   \n",
       "13        graph       copy         E1   e1_board4_words        algorithm   \n",
       "14      kitchen        egg         E1   e1_board4_words           eggnog   \n",
       "15         near  astronaut         E1   e1_board5_words           eggnog   \n",
       "16     homeless  apartment         E1   e1_board5_words           eggnog   \n",
       "17        onion  cigarette         E1   e1_board5_words           eggnog   \n",
       "18        pants     collar         E1   e1_board6_words           eggnog   \n",
       "19   definition     assist         E1   e1_board6_words           eggnog   \n",
       "20          war      quiet         E1   e1_board6_words           eggnog   \n",
       "21         flat      alike         E1   e1_board7_words           eggnog   \n",
       "22      perjury   adultery         E1   e1_board7_words           eggnog   \n",
       "23        heavy    feather         E1   e1_board7_words           eggnog   \n",
       "24       breeze     bubble         E1   e1_board8_words           eggnog   \n",
       "25          bad    actress         E1   e1_board8_words           eggnog   \n",
       "26  communicate    cooking         E1   e1_board8_words           eggnog   \n",
       "27          bat     bounce         E1   e1_board9_words           eggnog   \n",
       "28         toes    Dracula         E1   e1_board9_words           eggnog   \n",
       "29        lunch     almond         E1   e1_board9_words           eggnog   \n",
       "30         cave     knight         E2   e2_board1_words           eggnog   \n",
       "31        quick       glow         E2   e2_board1_words           eggnog   \n",
       "32         tree        oak         E2   e2_board1_words           eggnog   \n",
       "33       garage       bone         E2  e2_board10_words           eggnog   \n",
       "34         feet     chapel         E2  e2_board10_words           eggnog   \n",
       "35        chair      table         E2  e2_board10_words           eggnog   \n",
       "36        stern       wind         E2   e2_board2_words           eggnog   \n",
       "37          sun       bowl         E2   e2_board2_words           eggnog   \n",
       "38         gold     silver         E2   e2_board2_words           eggnog   \n",
       "39         holy       kind         E2   e2_board3_words           eggnog   \n",
       "40        teeth       gums         E2   e2_board3_words           eggnog   \n",
       "41         rude     regret         E2   e2_board3_words           eggnog   \n",
       "42        dream        bet         E2   e2_board4_words           eggnog   \n",
       "43         jump       leap         E2   e2_board4_words           eggnog   \n",
       "44       travel      ankle         E2   e2_board4_words           eggnog   \n",
       "45        snake        ash         E2   e2_board5_words           eggnog   \n",
       "46         hand      birth         E2   e2_board5_words           eggnog   \n",
       "47         lion      tiger         E2   e2_board5_words           eggnog   \n",
       "48          elm       rock         E2   e2_board6_words           eggnog   \n",
       "49        crust       boot         E2   e2_board6_words           eggnog   \n",
       "50          sit      stand         E2   e2_board6_words           eggnog   \n",
       "51         east      short         E2   e2_board7_words           Easter   \n",
       "52          old        new         E2   e2_board7_words           Easter   \n",
       "53        glass       cage         E2   e2_board7_words           Easter   \n",
       "54        giant     subtle         E2   e2_board8_words           Easter   \n",
       "55         army       drum         E2   e2_board8_words           Easter   \n",
       "56        happy        sad         E2   e2_board8_words           Easter   \n",
       "57        weird     trauma         E2   e2_board9_words           Easter   \n",
       "58        olive       real         E2   e2_board9_words           Easter   \n",
       "59       candle       wick         E2   e2_board9_words           Easter   \n",
       "\n",
       "   Similar Word Sum  \n",
       "0           volcano  \n",
       "1           volcano  \n",
       "2       examination  \n",
       "3       examination  \n",
       "4       examination  \n",
       "5       examination  \n",
       "6       examination  \n",
       "7       examination  \n",
       "8       examination  \n",
       "9       examination  \n",
       "10      examination  \n",
       "11      examination  \n",
       "12      examination  \n",
       "13      examination  \n",
       "14             kite  \n",
       "15             kite  \n",
       "16             kite  \n",
       "17             kite  \n",
       "18             kite  \n",
       "19             kite  \n",
       "20             kite  \n",
       "21             kite  \n",
       "22             kite  \n",
       "23             kite  \n",
       "24             kite  \n",
       "25             kite  \n",
       "26             kite  \n",
       "27             kite  \n",
       "28             kite  \n",
       "29             kite  \n",
       "30             kite  \n",
       "31             kite  \n",
       "32             kite  \n",
       "33             kite  \n",
       "34             kite  \n",
       "35             kite  \n",
       "36             kite  \n",
       "37             kite  \n",
       "38             kite  \n",
       "39             kite  \n",
       "40             kite  \n",
       "41             kite  \n",
       "42             kite  \n",
       "43             kite  \n",
       "44             kite  \n",
       "45             kite  \n",
       "46             kite  \n",
       "47             kite  \n",
       "48             kite  \n",
       "49             kite  \n",
       "50             kite  \n",
       "51         shortage  \n",
       "52         shortage  \n",
       "53         shortage  \n",
       "54         shortage  \n",
       "55         shortage  \n",
       "56         shortage  \n",
       "57         shortage  \n",
       "58         shortage  \n",
       "59         shortage  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_pair.head(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
